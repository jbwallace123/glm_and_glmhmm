{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Driven Observations (\"GLM-HMM\")\n",
    "\n",
    "Notebook adapted from Zoe Ashwood\n",
    "\n",
    "This notebook demonstrates the \"InputDrivenObservations\" class, and illustrates its use in the context of modeling decision-making data as in Ashwood et al. (2020) ([Mice alternate between discrete strategies during perceptual\n",
    "decision-making](https://www.biorxiv.org/content/10.1101/2020.10.19.346353v1.full.pdf)).\n",
    "\n",
    "Compared to the model considered in the notebook [\"2 Input Driven HMM\"](https://github.com/lindermanlab/ssm/blob/master/notebooks/2%20Input%20Driven%20HMM.ipynb), Ashwood et al. (2020) assumes a stationary transition matrix where transition probabilities *do not* depend on external inputs. However, observation probabilities now *do* depend on external covariates according to:\n",
    "\n",
    "\n",
    "for $c \\neq C$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = c \\mid z_{t} = k, u_t, w_{kc}) = \n",
    "\\frac{\\exp\\{w_{kc}^\\mathsf{T} u_t\\}}\n",
    "{1+\\sum_{c'=1}^{C-1} \\exp\\{w_{kc'}^\\mathsf{T} u_t\\}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and for $c = C$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = c \\mid z_{t} = k, u_t, w_{kc}) = \n",
    "\\frac{1}\n",
    "{1+\\sum_{c'=1}^{C-1} \\exp\\{w_{kc'}^\\mathsf{T} u_t\\}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $c \\in \\{1, ..., C\\}$ indicates the categorical class for the observation, $u_{t} \\in \\mathbb{R}^{M}$ is the set of input covariates, and $w_{kc} \\in \\mathbb{R}^{M}$ is the set of input weights associated with state $k$ and class $c$. These weights, along with the transition matrix and initial state probabilities, will be learned.\n",
    "\n",
    "In Ashwood et al. (2020), $C = 2$ as $y_{t}$ represents the binary choice made by an animal during a 2AFC (2-Alternative Forced Choice) task. The above equations then reduce to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = 0 \\mid z_{t} = k, u_t, w_{k}) = \n",
    "\\frac{\\exp\\{w_{k}^\\mathsf{T} u_t\\}}\n",
    "{1 + \\exp\\{w_{k}^\\mathsf{T} u_t\\}} = \\frac{1}\n",
    "{1 + \\exp\\{-w_{k}^\\mathsf{T} u_t\\}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = 1 \\mid z_{t} = k, u_t, w_{k}) = \n",
    "\\frac{1}\n",
    "{1 + \\exp\\{w_{k}^\\mathsf{T} u_t\\}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and only a single weight vector, $w_{k}$, is associated with each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "First, you must clone the `ssm` repository and install all of the dependencies. The `ssm` package we are using can be found, along with installation instructions, [here](https://github.com/lindermanlab/ssm.git). \n",
    "\n",
    "The line `import ssm` imports the package for use. Here, we have also imported a few other packages for plotting and splitting our data. \n",
    "\n",
    "### Data considerations: \n",
    "Your y data should be binary and indicated by 0 and 1. Your x data should be a matrix of shape (T, M) where T is the number of timepoints and M is the number of covariates and should be noted by -1, 0, and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sglm import hmmUtils, utils\n",
    "import ssm\n",
    "\n",
    "npr.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Import your data and filter based on any criteria you may have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = pd.read_csv(r'C:\\Users\\janet\\Documents\\Behavior_samp_data\\FreelyMoving_6nback_102323_wDOB_wrecordDF.csv')\n",
    "#filtered_data = pd.read_csv(r'C:\\Users\\janet\\Documents\\Behavior_samp_data\\HMM_6nback_102323_wDOB_wrecordDF.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = '80-20'\n",
    "filtered_data = data_[(data_['Condition'] == probs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Driven Observations\n",
    "We create a HMM with input-driven observations and 'standard' (stationary) transitions with the following line:  \n",
    "```python\n",
    "        ssm.HMM(num_states, obs_dim, input_dim, observations=\"input_driven_obs\", observation_kwargs=dict(C=num_categories), transitions=\"standard\")\n",
    "```\n",
    "\n",
    "As in Ashwood et al. (2020), we are going to model an animal's binary choice data during a decision-making task, so we will set `num_categories=2` because the animal only has two options available to it. We will also set `obs_dim = 1` because the dimensionality of the observation data is 1 (if we were also modeling, for example, the binned reaction time of the animal, we could set `obs_dim = 2`).  For the sake of simplicity, we will assume that an animal's choice in a particular state is only affected by the external stimulus associated with that particular trial, and its innate choice bias. Thus, we will set `input_dim = 2` and we will simulate input data that resembles sequences of stimuli in what follows.  In Ashwood et al. (2020), they found that many mice used 3 decision-making states when performing 2AFC tasks. We will, thus, set `num_states = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input-Driven Observations: This means that the observations you see (e.g., the weather you observe) are influenced by some external factors or covariates. For example, the probability of observing rain on a particular day might be influenced by the temperature or humidity on that day.\n",
    "\n",
    "Input-Driven Transitions: This means that the transition probabilities between different hidden states (e.g., different weather patterns) are influenced by external factors. For example, the likelihood of transitioning from a sunny day to a rainy day might depend on some external factors.\n",
    "\n",
    "Stationary Transitions: On the other hand, \"stationary transitions\" mean that the transitions between hidden states are fixed and do not depend on external inputs. In the context of the weather, this would mean that the probability of transitioning from one weather pattern to another is constant and doesn't change based on external factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we will be determining the optimal number of states - Let's loop through a list of num_states and see which one produces the highest log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Data and param set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = '80-20'\n",
    "train_split = 0.80\n",
    "seed = np.random.randint(1000)\n",
    "\n",
    "#filtered_data = data_[(data_['Condition'] == probs) & (data_['Mouse ID'] == 'GN')]\n",
    "\n",
    "## Get train/test session IDs\n",
    "\n",
    "train_ids, test_ids = train_test_split(filtered_data['Session ID'].unique(), \n",
    "                                       train_size=train_split, random_state=seed)\n",
    "\n",
    "print('You have {} training sessions and {} test sessions.'.format(len(train_ids), len(test_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new dataframes for train/test data\n",
    "\n",
    "train_data = filtered_data[filtered_data['Session ID'].isin(train_ids)]\n",
    "test_data = filtered_data[filtered_data['Session ID'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting specific columns and creating variables for model input\n",
    "x_cols = ['1_Reward', '2_Reward', '3_Reward', '1_Port', '2_Port', '3_Port', '1_ChoiceReward', '2_ChoiceReward', '3_ChoiceReward']\n",
    "\n",
    "#columns for x data\n",
    "train_data_x = train_data[x_cols]\n",
    "test_data_x = test_data[x_cols]\n",
    "\n",
    "#number of sessions and trials\n",
    "train_data_sessions = len(train_data_x)\n",
    "test_data_sessions = len(test_data_x)\n",
    "\n",
    "train_num_sess = len(train_data['Session ID'].unique())\n",
    "test_num_sess = len(test_data['Session ID'].unique())\n",
    "\n",
    "num_trials_per_train_sess = train_data_sessions\n",
    "num_trials_per_test_sess = test_data_sessions\n",
    "\n",
    "# Data inputs for y data \n",
    "choices = train_data['Decision'].to_numpy()\n",
    "choices = choices.reshape(-1, 1)\n",
    "choices = choices.astype(int)\n",
    "\n",
    "inpts = train_data_x.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inputs\n",
    "num_states = [1, 2, 3, 4] # number of discrete states\n",
    "obs_dim =  1 # number of observed dimensions, 1 for just reward, 2 if you had something like reaction time\n",
    "num_categories = 2 # number of categories for the output\n",
    "input_dim = inpts.shape[1]\n",
    "\n",
    "# C represents the binary choice the animal must make \n",
    "C = 2 \n",
    "\n",
    "# set sigma and alpha for the prior on the weights, 2 is a good starting point if you care about history dependence\n",
    "prior_sigma = 2\n",
    "prior_alpha = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Now, we will be running the models. We will be using the input driven observation model to predict the animal's choice on each trial, and then comparing that to the actual choice. We will then plot the prediction accuracy for each state. Importantly, we are changing our observation to 'input driven' and the transitions to 'standard'. We will also be performing a cross validation analysis to see how well the model generalizes to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_model_list = []\n",
    "stationary_ll_list = []\n",
    "stationary_train_scores = []\n",
    "stationary_test_scores = []\n",
    "for i in range(len(num_states)):\n",
    "    from ssm import model_selection\n",
    "    map_glmhmm = ssm.HMM(num_states[i], obs_dim, input_dim, observations=\"input_driven_obs\", \n",
    "                observation_kwargs=dict(C=num_categories, prior_sigma=prior_sigma),\n",
    "                transitions=\"standard\")\n",
    "    train_scores, test_scores = ssm.model_selection.cross_val_scores(map_glmhmm, choices, inpts, heldout_frac=0.1, n_repeats=5, verbose=True)\n",
    "    N_iters = 200\n",
    "    stationary_ll = map_glmhmm.fit(choices, inputs=inpts, method=\"em\", num_iters=N_iters, initialize=False)\n",
    "    stationary_ll_list.append(stationary_ll)\n",
    "    stationary_train_scores.append(train_scores)\n",
    "    stationary_test_scores.append(test_scores)\n",
    "    stationary_model_list.append(map_glmhmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your model outputs and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save stationary model list, train and test scores, and LL list\n",
    "import pickle\n",
    "\n",
    "model_params = {'num_states': num_states,\n",
    "                'obs_dim': obs_dim,\n",
    "                'input_dim': input_dim,\n",
    "                'num_categories': num_categories,\n",
    "                'input_dim': input_dim,\n",
    "                'C': C,\n",
    "                'prior_sigma': prior_sigma,\n",
    "                'prior_alpha': prior_alpha,\n",
    "                'observations': \"input_driven_obs\",\n",
    "                'x_cols': x_cols,\n",
    "                'notes': \"used 3 nback data, 80-20 prob, 80-20 split, 4 states, reward is 0-1, choice is -1-1, interaction is product of reward and choice\"}\n",
    "\n",
    "data_splits = {'train_split': train_split,\n",
    "               'train_ids': train_ids,\n",
    "               'test_ids': test_ids}\n",
    "\n",
    "#create dictionary for pickle\n",
    "stationary_dict = {'model_params' : model_params,\n",
    "                    'data_splits': data_splits,\n",
    "                   'stationary_model_list': stationary_model_list,\n",
    "                   'stationary_train_scores': stationary_train_scores,\n",
    "                   'stationary_test_scores': stationary_test_scores,\n",
    "                   'stationary_ll_list': stationary_ll_list}\n",
    "\n",
    "#save dictionary in pickle file\n",
    "save_dir = r'C:\\Users\\janet\\Documents\\Behavior_samp_data'\n",
    "pickle.dump(stationary_dict, open(os.path.join(save_dir, 'stationary_3nback_80-20_80-20split.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also re-load the saved model outputs and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model list, train and test scores, and LL list\n",
    "import pickle\n",
    "save_dir = r'C:\\Users\\janet\\Documents\\Behavior_samp_data'\n",
    "\n",
    "stationary_dict = pickle.load(open(os.path.join(save_dir, 'stationary_3nback_80-20_Reward_neg.pkl'), 'rb'))\n",
    "model_params = stationary_dict['model_params']\n",
    "stationary_model_list = stationary_dict['stationary_model_list']\n",
    "stationary_train_scores = stationary_dict['stationary_train_scores']\n",
    "stationary_test_scores = stationary_dict['stationary_test_scores']\n",
    "stationary_ll_list = stationary_dict['stationary_ll_list']\n",
    "\n",
    "num_states = model_params['num_states']\n",
    "obs_dim = model_params['obs_dim']\n",
    "input_dim = model_params['input_dim']\n",
    "num_categories = model_params['num_categories']\n",
    "input_dim = model_params['input_dim']\n",
    "C = model_params['C']\n",
    "prior_sigma = model_params['prior_sigma']\n",
    "prior_alpha = model_params['prior_alpha']\n",
    "observations = model_params['observations']\n",
    "notes = model_params['notes']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. QC your model outputs and plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calc standard error of the mean for train and test scores\n",
    "stationary_train_scores_sem = []\n",
    "for i in range(len(stationary_train_scores)):\n",
    "    stationary_train_scores_sem.append(np.std(stationary_train_scores[i])/np.sqrt(len(stationary_train_scores[i])))\n",
    "\n",
    "stationary_test_scores_sem = []\n",
    "for i in range(len(stationary_test_scores)):\n",
    "    stationary_test_scores_sem.append(np.std(stationary_test_scores[i])/np.sqrt(len(stationary_test_scores[i])))\n",
    "\n",
    "## determine critical value for set confidence interval\n",
    "from scipy.stats import norm\n",
    "confidence = 0.95\n",
    "alpha_2 = (1 - confidence) / 2\n",
    "critical_value = norm.ppf(1 - alpha_2)\n",
    "\n",
    "## calc confidence interval for train and test scores\n",
    "stationary_train_scores_ci = []\n",
    "for i in range(len(stationary_train_scores)):\n",
    "    stationary_train_scores_ci.append([np.mean(stationary_train_scores[i]) - (critical_value * stationary_train_scores_sem[i]),\n",
    "                                  np.mean(stationary_train_scores[i]) + (critical_value * stationary_train_scores_sem[i])])\n",
    "\n",
    "stationary_test_scores_ci = []\n",
    "for i in range(len(stationary_test_scores)):\n",
    "    stationary_test_scores_ci.append([np.mean(stationary_test_scores[i]) - (critical_value * stationary_test_scores_sem[i]),\n",
    "                                np.mean(stationary_test_scores[i]) + (critical_value * stationary_test_scores_sem[i])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot train and test scores for each model and display confidence intervals\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(num_states, [np.mean(s) for s in stationary_train_scores], label=\"Train\")\n",
    "plt.plot(num_states, [np.mean(s) for s in stationary_test_scores], label=\"Test\")\n",
    "plt.fill_between(num_states, [s[0] for s in stationary_train_scores_ci], [s[1] for s in stationary_train_scores_ci], alpha=0.3)\n",
    "plt.fill_between(num_states, [s[0] for s in stationary_test_scores_ci], [s[1] for s in stationary_test_scores_ci], alpha=0.3)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Number of states\")\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.title(\"Cross Validation Scores; error = confidence interval 95%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the log likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(stationary_model_list)):\n",
    "    log_likelihood = stationary_model_list[ii].log_likelihood(choices, inputs=inpts)\n",
    "    print('Model with ' + str(num_states[ii]) + ' states has a train log likelihood of ' + str(log_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##determine LL of held out test data\n",
    "test_choices = test_data['Decision'].to_numpy()\n",
    "test_choices = test_choices.reshape(-1, 1)\n",
    "test_choices = test_choices.astype(int)\n",
    "\n",
    "test_inpts = test_data_x.to_numpy()\n",
    "stationary_test_ll_list = []\n",
    "\n",
    "for ii in range(len(stationary_model_list)):\n",
    "    stationary_test_ll_list.append(stationary_model_list[ii].log_likelihood(test_choices, inputs=test_inpts))\n",
    "    print('Model with ' + str(num_states[ii]) + ' states has a test log likelihood of ' + str(stationary_test_ll_list[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalize log likelihood in test_glmhmm_list and test_ll_list\n",
    "stationary_norm_ll_list = []\n",
    "stationary_norm_test_ll_list = []\n",
    "for ii in range(len(stationary_ll_list)):\n",
    "    stationary_norm_ll_list.append([k / num_trials_per_train_sess for k in stationary_ll_list[ii] if k is not None])\n",
    "    stationary_norm_test_ll_list.append((stationary_test_ll_list[ii]/num_trials_per_test_sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot subplots of the log probabilities of the train and test models. \n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "for ii in range(len(stationary_ll_list)):\n",
    "    axs[ii].plot(stationary_norm_ll_list[ii], label=\"EM\")\n",
    "    axs[ii].plot([0, len(stationary_norm_ll_list[ii])], stationary_norm_test_ll_list[ii] * np.ones(2), ':k', label=\"Test\")\n",
    "    axs[ii].legend(loc=\"lower right\")\n",
    "    axs[ii].set_xlabel(\"EM Iteration\")\n",
    "    axs[ii].set_xlim(0, len(stationary_norm_ll_list[ii]))\n",
    "    axs[ii].set_ylabel(\"Log Probability\")\n",
    "    axs[ii].set_title('Model with ' + str(num_states[ii]) + ' states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert to bits per state\n",
    "stationary_bits_per_state_train = []\n",
    "for ii in range(len(stationary_model_list)):\n",
    "    import math\n",
    "    bits_per_sess = ((stationary_model_list[ii].log_likelihood(choices, inputs=inpts)-(np.log(0.5)*num_trials_per_train_sess))/(num_trials_per_train_sess*np.log(2)))\n",
    "    stationary_bits_per_state_train.append(bits_per_sess)\n",
    "\n",
    "stationary_bits_per_state_test = []\n",
    "for ii in range(len(stationary_model_list)):\n",
    "    import math\n",
    "    bits_per_sess = ((stationary_model_list[ii].log_likelihood(test_choices, inputs=test_inpts)-(np.log(0.5)*num_trials_per_test_sess))/(num_trials_per_test_sess*np.log(2)))\n",
    "    stationary_bits_per_state_test.append(bits_per_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot bits/state train vs test\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(num_states, stationary_bits_per_state_train, label=\"Train\", marker='o')\n",
    "plt.plot(num_states, stationary_bits_per_state_test, label=\"Test\", marker='o')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.ylabel(\"bits/trials\")\n",
    "plt.xticks(num_states)\n",
    "plt.title('Standard HMM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AIC for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate AIC  for train data normalized by number of trials\n",
    "stationary_AIC_train = []\n",
    "for ii in range(len(stationary_model_list)):\n",
    "    AIC = (-2*(stationary_model_list[ii].log_likelihood(choices, inputs=inpts)/num_trials_per_train_sess))+(2*(num_states[ii]))\n",
    "    stationary_AIC_train.append(AIC)\n",
    "\n",
    "stationary_AIC_test = []\n",
    "for ii in range(len(stationary_model_list)):\n",
    "    AIC = (-2*(stationary_model_list[ii].log_likelihood(test_choices, inputs=test_inpts)/num_trials_per_test_sess))+(2*(num_states[ii]))\n",
    "    stationary_AIC_test.append(AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot AIC for train and test data side by side\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "plt.plot(num_states, stationary_AIC_train, label=\"AIC Train\", marker='o', color='green')\n",
    "plt.plot(num_states, stationary_AIC_test, label=\"AIC Test\", marker='o', color='magenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.title('Standard HMM')\n",
    "plt.ylabel(\"AIC\")\n",
    "plt.xticks(num_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we will look at the expected states and the transition matrix and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get expected states for each model:\n",
    "expectations_list = []\n",
    "for i in range(len(num_states)):\n",
    "    expectations_list.append([stationary_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])])\n",
    "    \n",
    "## Get expected states for held out test data for each model:\n",
    "stationary_test_expectations_list = []\n",
    "for i in range(len(num_states)):\n",
    "    stationary_test_expectations_list.append([stationary_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([test_choices], [test_inpts])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicitve accuracy for test data for each model\n",
    "stationary_test_predictive_acc_list = []\n",
    "for i in range(len(num_states)):\n",
    "    model = stationary_model_list[i]\n",
    "    expectations = stationary_test_expectations_list[i]\n",
    "    glm_weights = -model.observations.params\n",
    "    permutation = np.argsort(glm_weights[:, 0, 0])\n",
    "    masks = [np.ones_like(data, dtype=bool) for data in [test_choices]]\n",
    "    # Convert this now to one array:\n",
    "    posterior_probs = np.concatenate(expectations, axis=0)\n",
    "    posterior_probs = posterior_probs[:, permutation]\n",
    "    prob_right = [\n",
    "        np.exp(model.observations.calculate_logits(input=test_inpts))\n",
    "        for data, input, train_mask in zip(test_choices, test_inpts, masks)\n",
    "    ]\n",
    "    prob_right = np.concatenate(prob_right, axis=0)\n",
    "    # Now multiply posterior probs and prob_right:\n",
    "    prob_right = prob_right[:, :, 1] ## taking logits running an exponential then arranging in a 3d array\n",
    "    # Now multiply posterior probs and prob_right and sum over latent axis:\n",
    "    final_prob_right = np.sum(np.multiply(posterior_probs, prob_right), axis=1)\n",
    "    # Get the predicted label for each time step:\n",
    "    predicted_label = np.around(final_prob_right, decimals=0).astype('int')\n",
    "    # Examine at appropriate idx\n",
    "    predictive_acc = np.sum(test_choices[:, 0] == predicted_label) / len(test_choices)\n",
    "    print('Model with ' + str(num_states[i]) + ' state(s) has a test predictive accuracy of ' + str(predictive_acc))\n",
    "    stationary_test_predictive_acc_list.append(predictive_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot plot of predictive accuracy for each model\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(num_states, stationary_test_predictive_acc_list, marker='o', color='magenta')\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.ylabel(\"Predictive Accuracy - Test Data\")\n",
    "plt.ylim(0.3, 1)\n",
    "plt.xticks(num_states)\n",
    "plt.title('Standard Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot GLM weights for your selected model (model_to_plot)\n",
    "num_states_2plot = 4\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "stationary_gen_weights = -stationary_model_list[model_to_plot].observations.params #select correct model whether in list, or just test_glmhmm\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8', '#e41a1c']\n",
    "# Plot the GLM weights for each input regressor against each state for the model with 2 states\n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for k in range(num_states_2plot):\n",
    "    plt.plot(stationary_gen_weights[k,0,:], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k], marker = 'o', markersize = 5)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n",
    "plt.title('Standard HMM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior State Probabilities\n",
    "Let's now plot $p(z_{t} = k|\\mathbf{y}, \\{u_{t}\\}_{t=1}^{T})$, the posterior state probabilities, which give the probability of the animal being in state k at trial t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all models to get posterior probabilities for train data and state_occupancies\n",
    "posterior_probs_list = []\n",
    "state_occupancies_list = []\n",
    "for i in range(len(num_states)):\n",
    "    posterior_probs = [stationary_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])]\n",
    "    posterior_probs_concat = np.concatenate(posterior_probs)\n",
    "    state_max_posterior = np.argmax(posterior_probs_concat, axis = 1)\n",
    "    _, state_occupancies = np.unique(state_max_posterior, return_counts=True)\n",
    "    state_occupancies = state_occupancies/np.sum(state_occupancies)\n",
    "    posterior_probs_list.append(posterior_probs)\n",
    "    state_occupancies_list.append(state_occupancies)\n",
    "\n",
    "#Loop through all models to get posterior probabilities for test data and state_occupancies\n",
    "stationary_test_posterior_probs_list = []\n",
    "stationary_test_state_occupancies_list = []\n",
    "for i in range(len(num_states)):\n",
    "    stationary_test_posterior_probs = [stationary_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([test_choices], [test_inpts])]\n",
    "    stationary_test_posterior_probs_concat = np.concatenate(stationary_test_posterior_probs)\n",
    "    stationary_test_state_max_posterior = np.argmax(stationary_test_posterior_probs_concat, axis = 1)\n",
    "    _, stationary_test_state_occupancies = np.unique(stationary_test_state_max_posterior, return_counts=True)\n",
    "    stationary_test_state_occupancies = stationary_test_state_occupancies/np.sum(stationary_test_state_occupancies)\n",
    "    stationary_test_posterior_probs_list.append(stationary_test_posterior_probs)\n",
    "    stationary_test_state_occupancies_list.append(stationary_test_state_occupancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot posterior state probabilities\n",
    "trials_to_plot = (0, 1000) # number of trials to plot\n",
    "num_states_2plot = 4\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "sess_id = 0 #session id; can choose any index between 0 and num_sess-1\n",
    "\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8', '#e41a1c']\n",
    "fig = plt.figure(figsize=(10, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "for k in range(num_states_2plot):\n",
    "    posterior_probs = posterior_probs_list[model_to_plot]\n",
    "    plt.plot(posterior_probs[sess_id][:, k], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k])\n",
    "plt.ylim((-0.01, 1.01))\n",
    "plt.yticks([0, 0.5, 1], fontsize = 10)\n",
    "plt.xlim(trials_to_plot) \n",
    "plt.xlabel(\"trial #\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"p(state)\", fontsize = 15)\n",
    "plt.title('Standard HMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fraction of time spent in each state for each model\n",
    "num_states_2plot = 4\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "fig = plt.figure(figsize=(2, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for z, occ in enumerate(stationary_test_state_occupancies_list[model_to_plot]):\n",
    "    plt.bar(z, occ, width = 0.8, color = cols[z])\n",
    "plt.ylim((0, 1))\n",
    "plt.xticks(range(num_states_2plot), range(num_states_2plot), fontsize = 10)\n",
    "plt.yticks([0, 0.5, 1], ['0', '0.5', '1'], fontsize=10)\n",
    "plt.xlabel('state', fontsize = 15)\n",
    "plt.ylabel('frac. occupancy', fontsize=15)\n",
    "plt.title('Standard HMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot transition matrix\n",
    "num_states_2plot = 4\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "gen_weights = stationary_model_list[model_to_plot].observations.params #select correct model whether in list, or just test_glmhmm\n",
    "gen_trans_mat = stationary_model_list[model_to_plot].transitions.params #select correct model whether in list, or just test_glmhmm\n",
    "\n",
    "\n",
    "# Plot generative parameters:\n",
    "fig = plt.figure(figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "gen_trans_mat = np.exp(gen_trans_mat)[0]\n",
    "plt.imshow(gen_trans_mat, vmin=-0.8, vmax=1, cmap='bone')\n",
    "for i in range(gen_trans_mat.shape[0]):\n",
    "    for j in range(gen_trans_mat.shape[1]):\n",
    "        text = plt.text(j, i, str(np.around(gen_trans_mat[i, j], decimals=2)), ha=\"center\", va=\"center\",\n",
    "                        color=\"k\", fontsize=12)\n",
    "plt.xlim(-0.5, num_states_2plot - 0.5)\n",
    "plt.xticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.yticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.ylim(num_states_2plot - 0.5, -0.5)\n",
    "plt.ylabel(\"state t\", fontsize = 15)\n",
    "plt.xlabel(\"state t+1\", fontsize = 15)\n",
    "plt.title(\"Generative transition matrix - Standard\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. We've been looping through a list of `num_states`, but we can also just select a single state and run with one state as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Fit GLM-HMM and perform recovery analysis -- this should come after the model selection section above.\n",
    "We will first make the model object, run cross validation, and then fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inputs\n",
    "num_states = 3 # number of discrete states\n",
    "obs_dim =  1 # number of observed dimensions, 1 for just reward, 2 if you had something like reaction time\n",
    "num_categories = 2 # number of categories for the output\n",
    "input_dim = inpts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_glmhmm = ssm.HMM(num_states, obs_dim, input_dim, observations=\"input_driven_obs\", \n",
    "                   observation_kwargs=dict(C=num_categories), transitions=\"standard\") # transitions usually standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform cross-validation\n",
    "from ssm import model_selection\n",
    "test_scores, train_scores = ssm.model_selection.cross_val_scores(new_glmhmm, choices, inpts, heldout_frac=0.1, n_repeats=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iters = 200 # maximum number of EM iterations. Fitting with stop earlier if increase in LL is below tolerance specified by tolerance parameter\n",
    "fit_ll = new_glmhmm.fit(choices, inputs=inpts, method='em', num_iters=N_iters, tolerance=10**-4) #method=\"em\",\n",
    "print(\"EM converged: \", fit_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit to test data\n",
    "test_choices = test_data['Decision'].to_numpy()\n",
    "test_choices = test_choices.reshape(-1, 1)\n",
    "test_choices = test_choices.astype(int)\n",
    "\n",
    "test_inpts = test_data_x.to_numpy()\n",
    "\n",
    "test_ll = new_glmhmm.log_likelihood(test_choices, inputs=test_inpts)\n",
    "print(\"Test LL: \", test_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the log likelihood\n",
    "norm_ll = [s / num_trials_per_train_sess for s in fit_ll if s is not None]\n",
    "norm_test = test_ll / num_trials_per_test_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log probabilities of the train and test models.\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(norm_ll, label=\"EM\")\n",
    "plt.plot([0, len(norm_ll)], norm_test * np.ones(2), ':k', label=\"Test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"EM Iteration\")\n",
    "plt.xlim(0, len(fit_ll))\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Posterior State Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = inpts.shape[0] # number of trials\n",
    "K = 2 # number of latent states\n",
    "C = 2 # number of observation classes or # C represents the binary choice the animal must make\n",
    "D = inpts.shape[1] # number of GLM inputs (regressors)\n",
    "\n",
    "# # Get expected states:\n",
    "posterior_probs = [new_glmhmm.expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_to_plot = (0, 300) # number of trials to plot\n",
    "\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8'] # colors for each state\n",
    "fig = plt.figure(figsize=(10, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "sess_id = 0 #session id; can choose any index between 0 and num_sess-1\n",
    "\n",
    "for k in range(num_states):\n",
    "    plt.plot(posterior_probs[sess_id][:, k], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k])\n",
    "plt.ylim((-0.01, 1.01))\n",
    "plt.yticks([0, 0.5, 1], fontsize = 10)\n",
    "plt.xlim(trials_to_plot) \n",
    "plt.xlabel(\"trial #\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"p(state)\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate posterior probabilities across sessions\n",
    "posterior_probs_concat = np.concatenate(posterior_probs)\n",
    "#Â get state with maximum posterior probability at particular trial:\n",
    "state_max_posterior = np.argmax(posterior_probs_concat, axis = 1)\n",
    "# now obtain state fractional occupancies:\n",
    "_, state_occupancies = np.unique(state_max_posterior, return_counts=True)\n",
    "state_occupancies = state_occupancies/np.sum(state_occupancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fraction of time spent in each state\n",
    "fig = plt.figure(figsize=(2, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for z, occ in enumerate(state_occupancies):\n",
    "    plt.bar(z, occ, width = 0.8, color = cols[z])\n",
    "plt.ylim((0, 1))\n",
    "plt.xticks([0, 1, 2], ['1', '2', '3'], fontsize = 10)\n",
    "plt.yticks([0, 0.5, 1], ['0', '0.5', '1'], fontsize=10)\n",
    "plt.xlabel('state', fontsize = 15)\n",
    "plt.ylabel('frac. occupancy', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GLM weights \n",
    "model_gen_weights = new_glmhmm.observations.params\n",
    "\n",
    "#Plot the weights for each input regressor against each state\n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for k in range(num_states):\n",
    "    plt.plot(model_gen_weights[k,0,:], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k], marker = 'o', markersize = 5)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot transition matrix\n",
    "num_states_2plot = num_states\n",
    "gen_weights = new_glmhmm.observations.params #select correct model whether in list, or just test_glmhmm\n",
    "gen_trans_mat = new_glmhmm.transitions.params #select correct model whether in list, or just test_glmhmm\n",
    "\n",
    "\n",
    "# Plot generative parameters:\n",
    "fig = plt.figure(figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "gen_trans_mat = np.exp(gen_trans_mat)[0]\n",
    "plt.imshow(gen_trans_mat, vmin=-0.8, vmax=1, cmap='bone')\n",
    "for i in range(gen_trans_mat.shape[0]):\n",
    "    for j in range(gen_trans_mat.shape[1]):\n",
    "        text = plt.text(j, i, str(np.around(gen_trans_mat[i, j], decimals=2)), ha=\"center\", va=\"center\",\n",
    "                        color=\"k\", fontsize=12)\n",
    "plt.xlim(-0.5, num_states_2plot - 0.5)\n",
    "plt.xticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.yticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.ylim(num_states_2plot - 0.5, -0.5)\n",
    "plt.ylabel(\"state t\", fontsize = 15)\n",
    "plt.xlabel(\"state t+1\", fontsize = 15)\n",
    "plt.title(\"Generative transition matrix\", fontsize = 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabatini-glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
