{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Driven Observations (\"GLM-HMM\")\n",
    "\n",
    "Notebook adapted from Zoe Ashwood\n",
    "\n",
    "This notebook demonstrates the \"InputDrivenObservations\" class, and illustrates its use in the context of modeling decision-making data as in Ashwood et al. (2020) ([Mice alternate between discrete strategies during perceptual\n",
    "decision-making](https://www.biorxiv.org/content/10.1101/2020.10.19.346353v1.full.pdf)).\n",
    "\n",
    "Compared to the model considered in the notebook [\"2 Input Driven HMM\"](https://github.com/lindermanlab/ssm/blob/master/notebooks/2%20Input%20Driven%20HMM.ipynb), Ashwood et al. (2020) assumes a stationary transition matrix where transition probabilities *do not* depend on external inputs. However, observation probabilities now *do* depend on external covariates according to:\n",
    "\n",
    "\n",
    "for $c \\neq C$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = c \\mid z_{t} = k, u_t, w_{kc}) = \n",
    "\\frac{\\exp\\{w_{kc}^\\mathsf{T} u_t\\}}\n",
    "{1+\\sum_{c'=1}^{C-1} \\exp\\{w_{kc'}^\\mathsf{T} u_t\\}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and for $c = C$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = c \\mid z_{t} = k, u_t, w_{kc}) = \n",
    "\\frac{1}\n",
    "{1+\\sum_{c'=1}^{C-1} \\exp\\{w_{kc'}^\\mathsf{T} u_t\\}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $c \\in \\{1, ..., C\\}$ indicates the categorical class for the observation, $u_{t} \\in \\mathbb{R}^{M}$ is the set of input covariates, and $w_{kc} \\in \\mathbb{R}^{M}$ is the set of input weights associated with state $k$ and class $c$. These weights, along with the transition matrix and initial state probabilities, will be learned.\n",
    "\n",
    "In Ashwood et al. (2020), $C = 2$ as $y_{t}$ represents the binary choice made by an animal during a 2AFC (2-Alternative Forced Choice) task. The above equations then reduce to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = 0 \\mid z_{t} = k, u_t, w_{k}) = \n",
    "\\frac{\\exp\\{w_{k}^\\mathsf{T} u_t\\}}\n",
    "{1 + \\exp\\{w_{k}^\\mathsf{T} u_t\\}} = \\frac{1}\n",
    "{1 + \\exp\\{-w_{k}^\\mathsf{T} u_t\\}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = 1 \\mid z_{t} = k, u_t, w_{k}) = \n",
    "\\frac{1}\n",
    "{1 + \\exp\\{w_{k}^\\mathsf{T} u_t\\}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and only a single weight vector, $w_{k}$, is associated with each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "First, you must clone the `ssm` repository and install all of the dependencies. The `ssm` package we are using can be found, along with installation instructions, [here](https://github.com/lindermanlab/ssm.git). \n",
    "\n",
    "The line `import ssm` imports the package for use. Here, we have also imported a few other packages for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sglm import hmmUtils, utils\n",
    "import ssm\n",
    "\n",
    "npr.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Below, we will be importing the data and filtering based on any feature you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = pd.read_csv(r'C:\\Users\\janet\\Documents\\Behavior_samp_data\\FreelyMoving_6nback_102323_wDOB_wrecordDF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1a.1. Define training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = '80-20'\n",
    "train_split = 0.7\n",
    "seed = np.random.randint(1000)\n",
    "\n",
    "filtered_data = data_[(data_['Condition'] == probs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_data.loc[:, 'Decision'] = filtered_data['Decision'].replace(0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get train/test session IDs\n",
    "\n",
    "train_ids, test_ids = train_test_split(filtered_data['Session ID'].unique(), \n",
    "                                       train_size=train_split, random_state=seed)\n",
    "\n",
    "print('You have {} training sessions and {} test sessions.'.format(len(train_ids), len(test_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements = np.array([s.split('_')[0] for s in test_ids])\n",
    "\n",
    "# Count and list unique elements\n",
    "unique_test_elements_count = np.unique(unique_elements).size\n",
    "unique_test_elements_list = np.unique(unique_elements)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of unique test elements: {unique_test_elements_count}\")\n",
    "print(f\"List of unique test elements: {unique_test_elements_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_r_elements = np.array([s.split('_')[0] for s in train_ids])\n",
    "\n",
    "# Count and list unique elements\n",
    "unique_train_elements_count = np.unique(unique_r_elements).size\n",
    "unique_train_elements_list = np.unique(unique_r_elements)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of unique train elements: {unique_train_elements_count}\")\n",
    "print(f\"List of unique train elements: {unique_train_elements_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of overlapping elements\n",
    "overlap = np.intersect1d(unique_r_elements, unique_elements)\n",
    "print(f\"Number of overlapping elements: {overlap.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new dataframes for train/test data\n",
    "\n",
    "train_data = filtered_data[filtered_data['Session ID'].isin(train_ids)]\n",
    "test_data = filtered_data[filtered_data['Session ID'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting specific columns and creating variables for model input\n",
    "x_cols = ['1_Port', '2_Port', '1_Reward', '2_Reward', '3_Port', '3_Reward']\n",
    "y_cols = ['Reward']\n",
    "\n",
    "#columns for x and y data\n",
    "train_data_x = train_data[x_cols]\n",
    "train_data_y = train_data[y_cols]\n",
    "test_data_x = test_data[x_cols]\n",
    "test_data_y = test_data[y_cols]\n",
    "\n",
    "#number of sessions and trials\n",
    "train_data_sessions = len(train_data_x)\n",
    "test_data_sessions = len(test_data_x)\n",
    "\n",
    "train_num_sess = len(train_data['Session ID'].unique())\n",
    "test_num_sess = len(test_data['Session ID'].unique())\n",
    "\n",
    "num_trials_per_train_sess = train_data_sessions\n",
    "num_trials_per_test_sess = test_data_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Driven Observations\n",
    "We create a HMM with input-driven observations and 'standard' (stationary) transitions with the following line:  \n",
    "```python\n",
    "        ssm.HMM(num_states, obs_dim, input_dim, observations=\"input_driven_obs\", observation_kwargs=dict(C=num_categories), transitions=\"standard\")\n",
    "```\n",
    "\n",
    "As in Ashwood et al. (2020), we are going to model an animal's binary choice data during a decision-making task, so we will set `num_categories=2` because the animal only has two options available to it. We will also set `obs_dim = 1` because the dimensionality of the observation data is 1 (if we were also modeling, for example, the binned reaction time of the animal, we could set `obs_dim = 2`).  For the sake of simplicity, we will assume that an animal's choice in a particular state is only affected by the external stimulus associated with that particular trial, and its innate choice bias. Thus, we will set `input_dim = 2` and we will simulate input data that resembles sequences of stimuli in what follows.  In Ashwood et al. (2020), they found that many mice used 3 decision-making states when performing 2AFC tasks. We will, thus, set `num_states = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input-Driven Observations: This means that the observations you see (e.g., the weather you observe) are influenced by some external factors or covariates. For example, the probability of observing rain on a particular day might be influenced by the temperature or humidity on that day.\n",
    "\n",
    "Input-Driven Transitions: This means that the transition probabilities between different hidden states (e.g., different weather patterns) are influenced by external factors. For example, the likelihood of transitioning from a sunny day to a rainy day might depend on some external factors.\n",
    "\n",
    "Stationary Transitions: On the other hand, \"stationary transitions\" mean that the transitions between hidden states are fixed and do not depend on external inputs. In the context of the weather, this would mean that the probability of transitioning from one weather pattern to another is constant and doesn't change based on external factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Fit GLM-HMM and perform recovery analysis -- this should come after the model selection section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data inputs \n",
    "choices = train_data['Decision'].to_numpy()\n",
    "choices = choices.reshape(-1, 1)\n",
    "choices = choices.astype(int)\n",
    "\n",
    "inpts = train_data_x.to_numpy()\n",
    "\n",
    "# Model Inputs\n",
    "num_states = 3 # number of discrete states\n",
    "obs_dim =  1 # number of observed dimensions, 1 for just reward, 2 if you had something like reaction time\n",
    "num_categories = 2 # number of categories for the output\n",
    "input_dim = inpts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_glmhmm = ssm.HMM(num_states, obs_dim, input_dim, observations=\"input_driven_obs\", \n",
    "                   observation_kwargs=dict(C=num_categories), transitions=\"standard\") # transitions usually standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iters = 500 # maximum number of EM iterations. Fitting with stop earlier if increase in LL is below tolerance specified by tolerance parameter\n",
    "fit_ll = new_glmhmm.fit(choices, inputs=inpts, method='em', num_iters=N_iters, tolerance=10**-4) #method=\"em\",\n",
    "print(\"EM converged: \", fit_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit to test data\n",
    "test_choices = test_data['Decision'].to_numpy()\n",
    "test_choices = test_choices.reshape(-1, 1)\n",
    "test_choices = test_choices.astype(int)\n",
    "\n",
    "test_inpts = test_data_x.to_numpy()\n",
    "\n",
    "test_ll = new_glmhmm.log_likelihood(test_choices, inputs=test_inpts)\n",
    "print(\"Test LL: \", test_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the log likelihood\n",
    "norm_ll = [s / num_trials_per_train_sess for s in fit_ll if s is not None]\n",
    "norm_test = test_ll / num_trials_per_test_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log probabilities of the train and test models. Fit model final LL should be greater \n",
    "# than or equal to true LL.\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(norm_ll, label=\"EM\")\n",
    "plt.plot([0, len(norm_ll)], norm_test * np.ones(2), ':k', label=\"Test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"EM Iteration\")\n",
    "plt.xlim(0, len(fit_ll))\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Posterior State Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot $p(z_{t} = k|\\mathbf{y}, \\{u_{t}\\}_{t=1}^{T})$, the posterior state probabilities, which give the probability of the animal being in state k at trial t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = inpts.shape[0] # number of trials\n",
    "K = 2 # number of latent states\n",
    "C = 2 # number of observation classes or # C represents the binary choice the animal must make\n",
    "D = inpts.shape[1] # number of GLM inputs (regressors)\n",
    "\n",
    "# # Get expected states:\n",
    "posterior_probs = [new_glmhmm.expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_to_plot = (0, 150) # number of trials to plot\n",
    "\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8']\n",
    "fig = plt.figure(figsize=(10, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "sess_id = 0 #session id; can choose any index between 0 and num_sess-1\n",
    "for k in range(num_states):\n",
    "    plt.plot(posterior_probs[sess_id][:, k], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k])\n",
    "plt.ylim((-0.01, 1.01))\n",
    "plt.yticks([0, 0.5, 1], fontsize = 10)\n",
    "plt.xlim(trials_to_plot) \n",
    "plt.xlabel(\"trial #\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"p(state)\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these posterior state probabilities, we can assign trials to states and then plot the fractional occupancy of each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate posterior probabilities across sessions\n",
    "posterior_probs_concat = np.concatenate(posterior_probs)\n",
    "# get state with maximum posterior probability at particular trial:\n",
    "state_max_posterior = np.argmax(posterior_probs_concat, axis = 1)\n",
    "# now obtain state fractional occupancies:\n",
    "_, state_occupancies = np.unique(state_max_posterior, return_counts=True)\n",
    "state_occupancies = state_occupancies/np.sum(state_occupancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for z, occ in enumerate(state_occupancies):\n",
    "    plt.bar(z, occ, width = 0.8, color = cols[z])\n",
    "plt.ylim((0, 1))\n",
    "plt.xticks([0, 1, 2], ['1', '2', '3'], fontsize = 10)\n",
    "plt.yticks([0, 0.5, 1], ['0', '0.5', '1'], fontsize=10)\n",
    "plt.xlabel('state', fontsize = 15)\n",
    "plt.ylabel('frac. occupancy', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gen_weights = new_glmhmm.observations.params\n",
    "\n",
    "#Plot the weights for each input regressor against each state\n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for k in range(num_states):\n",
    "    plt.plot(model_gen_weights[k,0,:], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k], marker = 'o', markersize = 5)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = num_states\n",
    "gen_weights = new_glmhmm.observations.params #select correct model whether in list, or just test_glmhmm\n",
    "gen_trans_mat = new_glmhmm.transitions.params #select correct model whether in list, or just test_glmhmm\n",
    "\n",
    "\n",
    "# Plot generative parameters:\n",
    "fig = plt.figure(figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "gen_trans_mat = np.exp(gen_trans_mat)[0]\n",
    "plt.imshow(gen_trans_mat, vmin=-0.8, vmax=1, cmap='bone')\n",
    "for i in range(gen_trans_mat.shape[0]):\n",
    "    for j in range(gen_trans_mat.shape[1]):\n",
    "        text = plt.text(j, i, str(np.around(gen_trans_mat[i, j], decimals=2)), ha=\"center\", va=\"center\",\n",
    "                        color=\"k\", fontsize=12)\n",
    "plt.xlim(-0.5, num_states_2plot - 0.5)\n",
    "plt.xticks(range(0, num_states_2plot), ('1', '2', '3'), fontsize=10)\n",
    "plt.yticks(range(0, num_states_2plot), ('1', '2', '3'), fontsize=10)\n",
    "plt.ylim(num_states_2plot - 0.5, -0.5)\n",
    "plt.ylabel(\"state t\", fontsize = 15)\n",
    "plt.xlabel(\"state t+1\", fontsize = 15)\n",
    "plt.title(\"Generative transition matrix\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[:, 'State_Occupancy'] = state_max_posterior # State_Max_Posterior contains the state for each given trial \n",
    "\n",
    "#save new csv\n",
    "#filtered_data.to_csv(r'C:\\Users\\janet\\Documents\\Behavior_samp_data\\FullMerged_2nBack_100423.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at train_data dataframe at trials where the state_occupancy switches\n",
    "explore_df = train_data.loc[train_data['State_Occupancy'].shift() != train_data['State_Occupancy']]\n",
    "explore_df = train_data.loc[train_data['State_Occupancy'].shift() != train_data['State_Occupancy']].index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Picking the optimal number of states - Let's loop through a list of num_states and see which one produces the highest log likelihood.\n",
    "\n",
    "We will be looping through a list of num_states and running the model on each. We will then plot the log likelihoods and see which number of states produces the highest log likelihood.\n",
    "\n",
    "To do: add cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = '80-20'\n",
    "train_split = 0.9\n",
    "seed = np.random.randint(1000)\n",
    "\n",
    "filtered_data = data_[(data_['Condition'] == probs)]\n",
    "\n",
    "## Get train/test session IDs\n",
    "\n",
    "train_ids, test_ids = train_test_split(filtered_data['Session ID'].unique(), \n",
    "                                       train_size=train_split, random_state=seed)\n",
    "\n",
    "print('You have {} training sessions and {} test sessions.'.format(len(train_ids), len(test_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new dataframes for train/test data\n",
    "\n",
    "train_data = filtered_data[filtered_data['Session ID'].isin(train_ids)]\n",
    "test_data = filtered_data[filtered_data['Session ID'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting specific columns and creating variables for model input\n",
    "x_cols = ['1_Port', '2_Port', '1_Reward', '2_Reward', '3_Port', '3_Reward']\n",
    "y_cols = ['Reward']\n",
    "\n",
    "#columns for x and y data\n",
    "train_data_x = train_data[x_cols]\n",
    "train_data_y = train_data[y_cols]\n",
    "test_data_x = test_data[x_cols]\n",
    "test_data_y = test_data[y_cols]\n",
    "\n",
    "#number of sessions and trials\n",
    "train_data_sessions = len(train_data_x)\n",
    "test_data_sessions = len(test_data_x)\n",
    "\n",
    "train_num_sess = len(train_data['Session ID'].unique())\n",
    "test_num_sess = len(test_data['Session ID'].unique())\n",
    "\n",
    "num_trials_per_train_sess = train_data_sessions\n",
    "num_trials_per_test_sess = test_data_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data inputs \n",
    "choices = train_data['Decision'].to_numpy()\n",
    "choices = choices.reshape(-1, 1)\n",
    "choices = choices.astype(int)\n",
    "\n",
    "inpts = train_data_x.to_numpy()\n",
    "\n",
    "\n",
    "# Model Inputs\n",
    "num_states = [1, 2, 3, 4] # number of discrete states\n",
    "obs_dim =  1 # number of observed dimensions, 1 for just reward, 2 if you had something like reaction time\n",
    "num_categories = 2 # number of categories for the output\n",
    "input_dim = inpts.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Now, we will be calculating the prediction accuracy of the model. We will be using the input driven observation model to predict the animal's choice on each trial, and then comparing that to the actual choice. We will then plot the prediction accuracy for each state. Importantly, we are changing our observation to 'input driven' and the transitions to 'sticky'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This needs to be done using the input driven observations class from Zoe Ashwood's code and Scott's code\n",
    "\n",
    "# C represents the binary choice the animal must make \n",
    "C = 2 \n",
    "\n",
    "# set sigma and alpha for the prior on the weights\n",
    "prior_sigma = 2\n",
    "prior_alpha = 2\n",
    "\n",
    "# Data inputs \n",
    "choices = train_data['Decision'].to_numpy()\n",
    "choices = choices.reshape(-1, 1)\n",
    "choices = choices.astype(int)\n",
    "\n",
    "inpts = train_data_x.to_numpy()\n",
    "\n",
    "# Model Inputs\n",
    "num_states = [1, 2, 3, 4] # number of discrete states\n",
    "obs_dim =  1 # number of observed dimensions, 1 for just reward, 2 if you had something like reaction time\n",
    "num_categories = 2 # number of categories for the output\n",
    "input_dim = inpts.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sticky_model_list = []\n",
    "sticky_ll_list = []\n",
    "for i in range(len(num_states)):\n",
    "    map_glmhmm = ssm.HMM(num_states[i], obs_dim, input_dim, observations=\"input_driven_obs\", \n",
    "                observation_kwargs=dict(C=num_categories,prior_sigma=prior_sigma),\n",
    "                transitions=\"sticky\", transition_kwargs=dict(alpha=prior_alpha,kappa=0))\n",
    "    N_iters = 200\n",
    "    sticky_ll = map_glmhmm.fit(choices, inputs=inpts, method=\"em\", num_iters=N_iters, initialize=False)\n",
    "    sticky_ll_list.append(sticky_ll)\n",
    "    sticky_model_list.append(map_glmhmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(sticky_model_list)):\n",
    "    log_likelihood = sticky_model_list[ii].log_likelihood(choices, inputs=inpts)\n",
    "    print('Model with ' + str(num_states[ii]) + ' states has a train log likelihood of ' + str(log_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##determine LL of held out test data\n",
    "test_choices = test_data['Decision'].to_numpy()\n",
    "test_choices = test_choices.reshape(-1, 1)\n",
    "test_choices = test_choices.astype(int)\n",
    "\n",
    "test_inpts = test_data_x.to_numpy()\n",
    "sticky_test_ll_list = []\n",
    "\n",
    "for ii in range(len(sticky_model_list)):\n",
    "    sticky_test_ll_list.append(sticky_model_list[ii].log_likelihood(test_choices, inputs=test_inpts))\n",
    "    print('Model with ' + str(num_states[ii]) + ' states has a test log likelihood of ' + str(sticky_test_ll_list[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalize log likelihood in test_glmhmm_list and test_ll_list\n",
    "sticky_norm_ll_list = []\n",
    "sticky_norm_test_ll_list = []\n",
    "for ii in range(len(sticky_ll_list)):\n",
    "    sticky_norm_ll_list.append([k / num_trials_per_train_sess for k in sticky_ll_list[ii] if k is not None])\n",
    "    sticky_norm_test_ll_list.append(sticky_test_ll_list[ii] / num_trials_per_test_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot subplots of the log probabilities of the train and test models. Fit model final LL should be greater\n",
    "# than or equal to true LL.\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "for ii in range(len(sticky_ll_list)):\n",
    "    axs[ii].plot(sticky_norm_ll_list[ii], label=\"EM\")\n",
    "    axs[ii].plot([0, len(sticky_norm_ll_list[ii])], sticky_norm_test_ll_list[ii] * np.ones(2), ':k', label=\"Test\")\n",
    "    axs[ii].legend(loc=\"lower right\")\n",
    "    axs[ii].set_xlabel(\"EM Iteration\")\n",
    "    axs[ii].set_xlim(0, len(sticky_norm_ll_list[ii]))\n",
    "    axs[ii].set_ylabel(\"Log Probability\")\n",
    "    axs[ii].set_title('Model with ' + str(num_states[ii]) + ' states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert to bits per state for train data\n",
    "sticky_bits_per_state_train = []\n",
    "for ii in range(len(sticky_model_list)):\n",
    "    import math\n",
    "    bits_per_sess = ((sticky_model_list[ii].log_likelihood(choices, inputs=inpts)-(np.log(0.5)*num_trials_per_train_sess))/(num_trials_per_train_sess*np.log(2)))\n",
    "    sticky_bits_per_state_train.append(bits_per_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert to bits per state for test data\n",
    "sticky_bits_per_state_test = []\n",
    "for ii in range(len(sticky_model_list)):\n",
    "    import math\n",
    "    bits_per_sess = ((sticky_model_list[ii].log_likelihood(test_choices, inputs=test_inpts)-(np.log(0.5)*num_trials_per_test_sess))/(num_trials_per_test_sess*np.log(2)))\n",
    "    sticky_bits_per_state_test.append(bits_per_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot bits/state train vs test\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(num_states, sticky_bits_per_state_train, label=\"Train\", marker='o')\n",
    "plt.plot(num_states, sticky_bits_per_state_test, label=\"Test\", marker='o')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.ylabel(\"bits/trials\")\n",
    "plt.xticks(num_states)\n",
    "plt.title('Sticky HMM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate AIC  for train data normalized by number of trials\n",
    "sticky_AIC_train = []\n",
    "for ii in range(len(sticky_model_list)):\n",
    "    AIC = (-2*(sticky_model_list[ii].log_likelihood(choices, inputs=inpts)/num_trials_per_train_sess))+(2*(num_states[ii]))\n",
    "    sticky_AIC_train.append(AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate AIC for test data normalized by number of trials\n",
    "sticky_AIC_test = []\n",
    "for ii in range(len(sticky_model_list)):\n",
    "    AIC = (-2*(sticky_model_list[ii].log_likelihood(test_choices, inputs=test_inpts)/num_trials_per_test_sess))+(2*(num_states[ii]))\n",
    "    sticky_AIC_test.append(AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot AIC for train and test data side by side\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "plt.plot(num_states, sticky_AIC_train, label=\"AIC Train\", marker='o', color='green')\n",
    "plt.plot(num_states, sticky_AIC_test, label=\"AIC Test\", marker='o', color='magenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.title('Sticky HMM')\n",
    "plt.ylabel(\"AIC\")\n",
    "plt.xticks(num_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get expected states for each model:\n",
    "expectations_list = []\n",
    "for i in range(len(num_states)):\n",
    "    expectations_list.append([sticky_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get expected states for held out test data for each model:\n",
    "sticky_test_expectations_list = []\n",
    "for i in range(len(num_states)):\n",
    "    sticky_test_expectations_list.append([sticky_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([test_choices], [test_inpts])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate predictive accuracy for each model\n",
    "predictive_acc_list = []\n",
    "for i in range(len(num_states)):\n",
    "    model = sticky_model_list[i]\n",
    "    expectations = expectations_list[i]\n",
    "    permutation = hmmUtils.calculate_state_permutation(model.params)\n",
    "    train_masks = [np.ones_like(data, dtype=bool) for data in [choices]]\n",
    "    # Convert this now to one array:\n",
    "    posterior_probs = np.concatenate(expectations, axis=0)\n",
    "    posterior_probs = posterior_probs[:, permutation]\n",
    "    prob_right = [\n",
    "        np.exp(model.observations.calculate_logits(input=inpts))\n",
    "        for data, input, train_mask in zip(choices, inpts, train_masks)\n",
    "    ]\n",
    "    prob_right = np.concatenate(prob_right, axis=0)\n",
    "    # Now multiply posterior probs and prob_right:\n",
    "    prob_right = prob_right[:, :, 1]\n",
    "    # Now multiply posterior probs and prob_right and sum over latent axis:\n",
    "    final_prob_right = np.sum(np.multiply(posterior_probs, prob_right), axis=1)\n",
    "    # Get the predicted label for each time step:\n",
    "    predicted_label = np.around(final_prob_right, decimals=0).astype('int')\n",
    "    # Examine at appropriate idx\n",
    "    predictive_acc = np.sum(choices[:, 0] == predicted_label) / len(choices)\n",
    "    predictive_acc_list.append(predictive_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate predictive accuracy for held out test data for each model\n",
    "sticky_test_predictive_acc_list = []\n",
    "for i in range(len(num_states)):\n",
    "    model = sticky_model_list[i]\n",
    "    expectations = sticky_test_expectations_list[i]\n",
    "    permutation = hmmUtils.calculate_state_permutation(model.params)\n",
    "    test_masks = [np.ones_like(data, dtype=bool) for data in [test_choices]]\n",
    "    # Convert this now to one array:\n",
    "    posterior_probs = np.concatenate(expectations, axis=0)\n",
    "    posterior_probs = posterior_probs[:, permutation]\n",
    "    prob_right = [\n",
    "        np.exp(model.observations.calculate_logits(input=test_inpts))\n",
    "        for data, input, test_mask in zip(test_choices, test_inpts, test_masks)\n",
    "    ]\n",
    "    prob_right = np.concatenate(prob_right, axis=0)\n",
    "    # Now multiply posterior probs and prob_right:\n",
    "    prob_right = prob_right[:, :, 1]\n",
    "    # Now multiply posterior probs and prob_right and sum over latent axis:\n",
    "    final_prob_right = np.sum(np.multiply(posterior_probs, prob_right), axis=1)\n",
    "    # Get the predicted label for each time step:\n",
    "    predicted_label = np.around(final_prob_right, decimals=0).astype('int')\n",
    "    # Examine at appropriate idx\n",
    "    predictive_acc = np.sum(test_choices[:, 0] == predicted_label) / len(test_choices)\n",
    "    sticky_test_predictive_acc_list.append(predictive_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot predictive accuracy for train and test data side by side\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "plt.plot(num_states, predictive_acc_list, label=\"Train\", marker='o', color='green')\n",
    "plt.plot(num_states, sticky_test_predictive_acc_list, label=\"Test\", marker='o', color='magenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.ylabel(\"Predictive Accuracy\")\n",
    "plt.xticks(num_states)\n",
    "plt.title('Sticky Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = 3\n",
    "sticky_gen_weights = sticky_model_list[2].observations.params #select correct model whether in list, or just test_glmhmm\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8']\n",
    "# Plot the GLM weights for each input regressor against each state for the model with 2 states\n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for k in range(num_states_2plot):\n",
    "    plt.plot(sticky_gen_weights[k,0,:], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k], marker = 'o', markersize = 5)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting transition matrix: Here, we will look at the resulting transition matrix. You can select any model output from the list below to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = 3\n",
    "gen_weights = sticky_model_list[2].observations.params #select correct model whether in list, or just test_glmhmm\n",
    "gen_trans_mat = sticky_model_list[2].transitions.params #select correct model whether in list, or just test_glmhmm\n",
    "\n",
    "\n",
    "# Plot generative parameters:\n",
    "fig = plt.figure(figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "gen_trans_mat = np.exp(gen_trans_mat)[0]\n",
    "plt.imshow(gen_trans_mat, vmin=-0.8, vmax=1, cmap='bone')\n",
    "for i in range(gen_trans_mat.shape[0]):\n",
    "    for j in range(gen_trans_mat.shape[1]):\n",
    "        text = plt.text(j, i, str(np.around(gen_trans_mat[i, j], decimals=2)), ha=\"center\", va=\"center\",\n",
    "                        color=\"k\", fontsize=12)\n",
    "plt.xlim(-0.5, num_states_2plot - 0.5)\n",
    "plt.xticks(range(0, num_states_2plot), ('1', '2', '3'), fontsize=10)\n",
    "plt.yticks(range(0, num_states_2plot), ('1', '2', '3'), fontsize=10)\n",
    "plt.ylim(num_states_2plot - 0.5, -0.5)\n",
    "plt.ylabel(\"state t\", fontsize = 15)\n",
    "plt.xlabel(\"state t+1\", fontsize = 15)\n",
    "plt.title(\"Generative transition matrix\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = inpts.shape[0] # number of trials\n",
    "K = 2 # number of latent states\n",
    "C = 2 # number of observation classes or # C represents the binary choice the animal must make\n",
    "D = inpts.shape[1] # number of GLM inputs (regressors)\n",
    "\n",
    "# # Get expected states:\n",
    "posterior_probs = [sticky_model_list[1].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate posterior probabilities across sessions\n",
    "posterior_probs_concat = np.concatenate(posterior_probs)\n",
    "# get state with maximum posterior probability at particular trial:\n",
    "state_max_posterior = np.argmax(posterior_probs_concat, axis = 1)\n",
    "# now obtain state fractional occupancies:\n",
    "_, state_occupancies = np.unique(state_max_posterior, return_counts=True)\n",
    "state_occupancies = state_occupancies/np.sum(state_occupancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_to_plot = (0, 200) # number of trials to plot\n",
    "\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8']\n",
    "fig = plt.figure(figsize=(10, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "sess_id = 0 #session id; can choose any index between 0 and num_sess-1\n",
    "for k in range(2):\n",
    "    plt.plot(posterior_probs[sess_id][:, k], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k])\n",
    "plt.ylim((-0.01, 1.01))\n",
    "plt.yticks([0, 0.5, 1], fontsize = 10)\n",
    "plt.xlim(trials_to_plot) \n",
    "plt.xlabel(\"trial #\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"p(state)\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for z, occ in enumerate(state_occupancies):\n",
    "    plt.bar(z, occ, width = 0.8, color = cols[z])\n",
    "plt.ylim((0, 1))\n",
    "plt.xticks([0, 1, 2], ['1', '2', '3'], fontsize = 10)\n",
    "plt.yticks([0, 0.5, 1], ['0', '0.5', '1'], fontsize=10)\n",
    "plt.xlabel('state', fontsize = 15)\n",
    "plt.ylabel('frac. occupancy', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabatini-glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
