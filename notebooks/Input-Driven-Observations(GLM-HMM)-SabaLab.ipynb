{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Driven Observations (\"GLM-HMM\")\n",
    "\n",
    "Notebook adapted from Zoe Ashwood\n",
    "\n",
    "This notebook demonstrates the \"InputDrivenObservations\" class, and illustrates its use in the context of modeling decision-making data as in Ashwood et al. (2020) ([Mice alternate between discrete strategies during perceptual\n",
    "decision-making](https://www.biorxiv.org/content/10.1101/2020.10.19.346353v1.full.pdf)).\n",
    "\n",
    "Compared to the model considered in the notebook [\"2 Input Driven HMM\"](https://github.com/lindermanlab/ssm/blob/master/notebooks/2%20Input%20Driven%20HMM.ipynb), Ashwood et al. (2020) assumes a stationary transition matrix where transition probabilities *do not* depend on external inputs. However, observation probabilities now *do* depend on external covariates according to:\n",
    "\n",
    "\n",
    "for $c \\neq C$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = c \\mid z_{t} = k, u_t, w_{kc}) = \n",
    "\\frac{\\exp\\{w_{kc}^\\mathsf{T} u_t\\}}\n",
    "{1+\\sum_{c'=1}^{C-1} \\exp\\{w_{kc'}^\\mathsf{T} u_t\\}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and for $c = C$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = c \\mid z_{t} = k, u_t, w_{kc}) = \n",
    "\\frac{1}\n",
    "{1+\\sum_{c'=1}^{C-1} \\exp\\{w_{kc'}^\\mathsf{T} u_t\\}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $c \\in \\{1, ..., C\\}$ indicates the categorical class for the observation, $u_{t} \\in \\mathbb{R}^{M}$ is the set of input covariates, and $w_{kc} \\in \\mathbb{R}^{M}$ is the set of input weights associated with state $k$ and class $c$. These weights, along with the transition matrix and initial state probabilities, will be learned.\n",
    "\n",
    "In Ashwood et al. (2020), $C = 2$ as $y_{t}$ represents the binary choice made by an animal during a 2AFC (2-Alternative Forced Choice) task. The above equations then reduce to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = 0 \\mid z_{t} = k, u_t, w_{k}) = \n",
    "\\frac{\\exp\\{w_{k}^\\mathsf{T} u_t\\}}\n",
    "{1 + \\exp\\{w_{k}^\\mathsf{T} u_t\\}} = \\frac{1}\n",
    "{1 + \\exp\\{-w_{k}^\\mathsf{T} u_t\\}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(y_t = 1 \\mid z_{t} = k, u_t, w_{k}) = \n",
    "\\frac{1}\n",
    "{1 + \\exp\\{w_{k}^\\mathsf{T} u_t\\}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and only a single weight vector, $w_{k}$, is associated with each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "First, you must clone the `ssm` repository and install all of the dependencies. The `ssm` package we are using can be found, along with installation instructions, [here](https://github.com/lindermanlab/ssm.git). \n",
    "\n",
    "The line `import ssm` imports the package for use. Here, we have also imported a few other packages for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sglm import hmmUtils, utils\n",
    "import ssm\n",
    "\n",
    "npr.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Import your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = pd.read_csv(r'C:\\Users\\janet\\Documents\\Behavior_samp_data\\FreelyMoving_6nback_102323_wDOB_wrecordDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = '80-20'\n",
    "ages = [1, 2, 3, 4] # 1: 2 months, 2: 4 months, 3: 6 months, 4: 12 months\n",
    "trial_based = 1\n",
    "filtered_data = data_[(data_['Condition'] == probs) & (data_['Age_Group'].isin(ages)) & (data_['Trial_based'] == trial_based)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choice as -1 or 1\n",
    "filtered_data.loc[:, '1_Port'] = filtered_data['1_Port'].replace(0, -1)\n",
    "filtered_data.loc[:, '2_Port'] = filtered_data['2_Port'].replace(0, -1)\n",
    "filtered_data.loc[:, '3_Port'] = filtered_data['3_Port'].replace(0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add columns for interaction between reward and choice\n",
    "filtered_data.loc[:, '1_ChoiceReward'] = filtered_data['1_Port'] * filtered_data['1_Reward']\n",
    "filtered_data.loc[:, '2_ChoiceReward'] = filtered_data['2_Port'] * filtered_data['2_Reward']\n",
    "filtered_data.loc[:, '3_ChoiceReward'] = filtered_data['3_Port'] * filtered_data['3_Reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add up animals in unique age groups\n",
    "age_group = filtered_data.groupby('Age_Group')\n",
    "\n",
    "#add up in each age group\n",
    "age_group_counts = age_group['Mouse ID'].unique()\n",
    "age_group_counts = age_group_counts.apply(lambda x: len(x))\n",
    "age_group_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_group_counts = age_group['Mouse ID'].unique()\n",
    "age_group_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Driven Observations\n",
    "We create a HMM with input-driven observations and 'standard' (stationary) transitions with the following line:  \n",
    "```python\n",
    "        ssm.HMM(num_states, obs_dim, input_dim, observations=\"input_driven_obs\", observation_kwargs=dict(C=num_categories), transitions=\"standard\")\n",
    "```\n",
    "\n",
    "As in Ashwood et al. (2020), we are going to model an animal's binary choice data during a decision-making task, so we will set `num_categories=2` because the animal only has two options available to it. We will also set `obs_dim = 1` because the dimensionality of the observation data is 1 (if we were also modeling, for example, the binned reaction time of the animal, we could set `obs_dim = 2`).  For the sake of simplicity, we will assume that an animal's choice in a particular state is only affected by the external stimulus associated with that particular trial, and its innate choice bias. Thus, we will set `input_dim = 2` and we will simulate input data that resembles sequences of stimuli in what follows.  In Ashwood et al. (2020), they found that many mice used 3 decision-making states when performing 2AFC tasks. We will, thus, set `num_states = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input-Driven Observations: This means that the observations you see (e.g., the weather you observe) are influenced by some external factors or covariates. For example, the probability of observing rain on a particular day might be influenced by the temperature or humidity on that day.\n",
    "\n",
    "Input-Driven Transitions: This means that the transition probabilities between different hidden states (e.g., different weather patterns) are influenced by external factors. For example, the likelihood of transitioning from a sunny day to a rainy day might depend on some external factors.\n",
    "\n",
    "Stationary Transitions: On the other hand, \"stationary transitions\" mean that the transitions between hidden states are fixed and do not depend on external inputs. In the context of the weather, this would mean that the probability of transitioning from one weather pattern to another is constant and doesn't change based on external factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be Picking the optimal number of states - Let's loop through a list of num_states and see which one produces the highest log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Data and param set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = '80-20'\n",
    "train_split = 0.90\n",
    "seed = np.random.randint(1000)\n",
    "\n",
    "# Get train/test session IDs\n",
    "\n",
    "train_ids, test_ids = train_test_split(filtered_data['Session ID'].unique(), \n",
    "                                      train_size=train_split, random_state=seed)\n",
    "\n",
    "print('You have {} training sessions and {} test sessions.'.format(len(train_ids), len(test_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create separate datasets for each mouse\n",
    "mice = filtered_data['Mouse ID'].unique()\n",
    "\n",
    "mouse_dfs = []\n",
    "\n",
    "for mouse in mice:\n",
    "    mouse_data = filtered_data[filtered_data['Mouse ID'] == mouse]\n",
    "    mouse_dfs.append({'mouse': mouse, 'data': mouse_data})    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split each mouse's data into train and test sets\n",
    "train_split = 0.9\n",
    "seed = np.random.randint(1000)\n",
    "\n",
    "#get train and test session IDs\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "splits = []\n",
    "\n",
    "for mouse in mouse_dfs:\n",
    "    mouse_train, mouse_test = train_test_split(mouse['data']['Session ID'].unique(), \n",
    "                                               train_size=train_split, random_state=seed)\n",
    "    train_ids.append(mouse_train)\n",
    "    test_ids.append(mouse_test)\n",
    "    splits.append({'mouse': mouse['mouse'], 'train': len(mouse_train), 'test': len(mouse_test)})\n",
    "\n",
    "#split data\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i, mouse in enumerate(mouse_dfs):\n",
    "    train_data.append({'mouse': mouse, 'data': mouse['data'][mouse['data']['Session ID'].isin(train_ids[i])]})\n",
    "    test_data.append({'mouse': mouse, 'data': mouse['data'][mouse['data']['Session ID'].isin(test_ids[i])]})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframes for train/test data\n",
    "\n",
    "train_data = filtered_data[filtered_data['Session ID'].isin(train_ids)]\n",
    "test_data = filtered_data[filtered_data['Session ID'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_elements = np.array([s.split('_')[0] for s in test_ids])\n",
    "\n",
    "# # Count and list unique elements\n",
    "# unique_test_elements_count = np.unique(unique_elements).size\n",
    "# unique_test_elements_list = np.unique(unique_elements)\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"Number of unique test elements: {unique_test_elements_count}\")\n",
    "# print(f\"List of unique test elements: {unique_test_elements_list}\")\n",
    "\n",
    "# unique_r_elements = np.array([s.split('_')[0] for s in train_ids])\n",
    "\n",
    "# # Count and list unique elements\n",
    "# unique_train_elements_count = np.unique(unique_r_elements).size\n",
    "# unique_train_elements_list = np.unique(unique_r_elements)\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"Number of unique train elements: {unique_train_elements_count}\")\n",
    "# print(f\"List of unique train elements: {unique_train_elements_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of overlapping elements\n",
    "#overlap = np.intersect1d(unique_r_elements, unique_elements)\n",
    "#print(f\"Number of overlapping elements: {overlap.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting specific columns and creating variables for model input GLOBAL\n",
    "x_cols = ['1_Reward', '2_Reward', '3_Reward', '1_Port', '2_Port', '3_Port', '1_ChoiceReward', '2_ChoiceReward', '3_ChoiceReward']\n",
    "\n",
    "#columns for x and y data\n",
    "train_data_x = train_data[x_cols]\n",
    "test_data_x = test_data[x_cols]\n",
    "\n",
    "#number of sessions and trials\n",
    "train_data_sessions = len(train_data_x)\n",
    "test_data_sessions = len(test_data_x)\n",
    "\n",
    "train_num_sess = len(train_data['Session ID'].unique())\n",
    "test_num_sess = len(test_data['Session ID'].unique())\n",
    "\n",
    "num_trials_per_train_sess = train_data_sessions\n",
    "num_trials_per_test_sess = test_data_sessions\n",
    "\n",
    "# Data inputs \n",
    "choices = train_data['Decision'].to_numpy()\n",
    "choices = choices.reshape(-1, 1)\n",
    "choices = choices.astype(int)\n",
    "\n",
    "inpts = train_data_x.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through each mouse to extract data and set model inputs - INDIVIDUAL\n",
    "x_cols = ['1_Reward', '2_Reward', '3_Reward', '1_Port', '2_Port', '3_Port', '1_ChoiceReward', '2_ChoiceReward', '3_ChoiceReward']\n",
    "\n",
    "#create lists of inputs for each mouse \n",
    "train_data_x = []\n",
    "test_data_x = []\n",
    "train_data_sessions = []\n",
    "test_data_sessions = []\n",
    "train_choices = []\n",
    "test_choices = []\n",
    "\n",
    "for i, mouse in enumerate(mouse_dfs):\n",
    "    train_data_x.append({'mouse': train_data[i]['mouse']['mouse'], 'data': train_data[i]['data'][x_cols]})\n",
    "    test_data_x.append({'mouse': test_data[i]['mouse']['mouse'], 'data': test_data[i]['data'][x_cols]})\n",
    "    train_data_sessions.append({'mouse': train_data[i]['mouse']['mouse'], 'sessions': len(train_data[i]['data'])})\n",
    "    test_data_sessions.append({'mouse': test_data[i]['mouse']['mouse'], 'sessions': len(test_data[i]['data'])})\n",
    "    train_choices.append({'mouse': train_data[i]['mouse']['mouse'], 'choices': train_data[i]['data']['Decision'].to_numpy().reshape(-1, 1).astype(int)})\n",
    "    test_choices.append({'mouse': test_data[i]['mouse']['mouse'], 'choices': test_data[i]['data']['Decision'].to_numpy().reshape(-1, 1).astype(int)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inputs\n",
    "num_states = [2] # number of discrete states\n",
    "obs_dim =  1 # number of observed dimensions, 1 for just reward, 2 if you had something like reaction time\n",
    "num_categories = 2 # number of categories for the output\n",
    "#input_dim = inpts.shape[1]\n",
    "\n",
    "# C represents the binary choice the animal must make \n",
    "C = 2 \n",
    "\n",
    "# set sigma and alpha for the prior on the weights\n",
    "prior_sigma = 2\n",
    "prior_alpha = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Option 1 - Now, we will be running the models. We will be using the input driven observation model to predict the animal's choice on each trial, and then comparing that to the actual choice. We will then plot the prediction accuracy for each state. Importantly, we are changing our observation to 'input driven' and the transitions to 'sticky'. We will also be performing a cross validation analysis to see how well the model generalizes to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random \n",
    "\n",
    "hmmUtils.wandb_login()\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"bandit_rl\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"architecture\": \"GLM-HMM\",\n",
    "    \"dataset\": \"Kevin Bandit Data\",\n",
    "    \"num_states\": num_states,\n",
    "    \"tranisitions\": \"sticky\",\n",
    "    \"iterations\": 200,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each mouse to fit the model and log results\n",
    "sticky_model_list = []\n",
    "sticky_ll_list = []\n",
    "sticky_train_scores = []\n",
    "sticky_test_scores = []\n",
    "\n",
    "for i, mouse in enumerate(mouse_dfs):\n",
    "    #set inputs\n",
    "    inpts = train_data_x[i]['data'].to_numpy()\n",
    "    choices = train_choices[i]['choices']\n",
    "    num_trials_per_sess = len(inpts)\n",
    "    num_sessions = len(train_data_x[i]['data'])\n",
    "    input_dim = inpts.shape[1]\n",
    "    \n",
    "    # fit the model\n",
    "    for i in range(len(num_states)):\n",
    "        from ssm import model_selection\n",
    "        map_glmhmm = ssm.HMM(num_states[i], obs_dim, input_dim, observations=\"input_driven_obs\", \n",
    "                    observation_kwargs=dict(C=num_categories,prior_sigma=prior_sigma),\n",
    "                    transitions=\"sticky\", transition_kwargs=dict(alpha=prior_alpha,kappa=0))\n",
    "        train_scores, test_scores = ssm.model_selection.cross_val_scores(map_glmhmm, choices, inpts, heldout_frac=0.1, n_repeats=3, verbose=True)\n",
    "        N_iters = 500\n",
    "        sticky_ll = map_glmhmm.fit(choices, inputs=inpts, method=\"em\", num_iters=N_iters, initialize=False)\n",
    "        sticky_ll_list.append({'mouse': train_data[i]['mouse']['mouse'], 'll': sticky_ll})\n",
    "        sticky_train_scores.append({'mouse': train_data[i]['mouse']['mouse'], 'scores': train_scores})\n",
    "        sticky_test_scores.append({'mouse': test_data[i]['mouse']['mouse'], 'scores':test_scores})\n",
    "        sticky_model_list.append({'mouse': train_data[i]['mouse']['mouse'], 'glmhmm': map_glmhmm})\n",
    "    \n",
    "    # # log results\n",
    "    # wandb.log({\"mouse\": mouse['mouse'],\n",
    "    #            \"num_sessions\": num_sessions,\n",
    "    #            \"num_trials_per_sess\": num_trials_per_sess,\n",
    "    #            \"num_states\": num_states,\n",
    "    #            \"obs_dim\": obs_dim,\n",
    "    #            \"input_dim\": input_dim,\n",
    "    #            \"num_categories\": num_categories,\n",
    "    #            \"C\": C,\n",
    "    #            \"prior_sigma\": prior_sigma,\n",
    "    #            \"prior_alpha\": prior_alpha, \n",
    "    #            \"LL\": sticky_ll_list,\n",
    "    #           })\n",
    "    \n",
    "    \n",
    "    # # log the model\n",
    "    # #wandb.save(wandb.run.dir + f\"/model_{mouse['mouse']}.h5\")\n",
    "    # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stickyG_model_list = []\n",
    "stickyG_ll_list = []\n",
    "stickyG_train_scores = []\n",
    "stickyG_test_scores = []\n",
    "for i in range(len(num_states)):\n",
    "    from ssm import model_selection\n",
    "    map_glmhmm = ssm.HMM(num_states[i], obs_dim, input_dim, observations=\"input_driven_obs\", \n",
    "                observation_kwargs=dict(C=num_categories,prior_sigma=prior_sigma),\n",
    "                transitions=\"sticky\", transition_kwargs=dict(alpha=prior_alpha,kappa=0))\n",
    "    train_scores, test_scores = ssm.model_selection.cross_val_scores(map_glmhmm, choices, inpts, heldout_frac=0.1, n_repeats=3, verbose=True)\n",
    "    N_iters = 500\n",
    "    sticky_ll = map_glmhmm.fit(choices, inputs=inpts, method=\"em\", num_iters=N_iters, initialize=False)\n",
    "    stickyG_ll_list.append(sticky_ll)\n",
    "    stickyG_train_scores.append(train_scores)\n",
    "    stickyG_test_scores.append(test_scores)\n",
    "    stickyG_model_list.append(map_glmhmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calc standard error of the mean for train and test scores\n",
    "stickyG_train_scores_sem = []\n",
    "for i in range(len(stickyG_train_scores)):\n",
    "    stickyG_train_scores_sem.append(np.std(stickyG_train_scores[i])/np.sqrt(len(stickyG_train_scores[i])))\n",
    "\n",
    "stickyG_test_scores_sem = []\n",
    "for i in range(len(stickyG_test_scores)):\n",
    "    stickyG_test_scores_sem.append(np.std(stickyG_test_scores[i])/np.sqrt(len(stickyG_test_scores[i])))\n",
    "\n",
    "## determine critical value for set confidence interval\n",
    "from scipy.stats import norm\n",
    "confidence = 0.95\n",
    "alpha_2 = (1 - confidence) / 2\n",
    "critical_value = norm.ppf(1 - alpha_2)\n",
    "\n",
    "## calc confidence interval for train and test scores\n",
    "stickyG_train_scores_ci = []\n",
    "for i in range(len(stickyG_train_scores)):\n",
    "    stickyG_train_scores_ci.append([np.mean(stickyG_train_scores[i]) - (critical_value * stickyG_train_scores_sem[i]),\n",
    "                                  np.mean(stickyG_train_scores[i]) + (critical_value * stickyG_train_scores_sem[i])])\n",
    "\n",
    "stickyG_test_scores_ci = []\n",
    "for i in range(len(stickyG_test_scores)):\n",
    "    stickyG_test_scores_ci.append([np.mean(stickyG_test_scores[i]) - (critical_value * stickyG_test_scores_sem[i]),\n",
    "                                np.mean(stickyG_test_scores[i]) + (critical_value * stickyG_test_scores_sem[i])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot train and test scores for each model and display confidence intervals\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(num_states, [np.mean(s) for s in stickyG_train_scores], label=\"Train\")\n",
    "plt.plot(num_states, [np.mean(s) for s in stickyG_test_scores], label=\"Test\")\n",
    "plt.fill_between(num_states, [s[0] for s in stickyG_train_scores_ci], [s[1] for s in stickyG_train_scores_ci], alpha=0.3)\n",
    "plt.fill_between(num_states, [s[0] for s in stickyG_test_scores_ci], [s[1] for s in stickyG_test_scores_ci], alpha=0.3)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Number of states\")\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.title(\"Cross Validation Scores; error = confidence interval 95%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(stickyG_model_list)):\n",
    "    log_likelihood = stickyG_model_list[ii].log_likelihood(choices, inputs=inpts)\n",
    "    print('Model with ' + str(num_states[ii]) + ' states has a train log likelihood of ' + str(log_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop through each mouse to determine test log likelihood\n",
    "sticky_test_ll = []\n",
    "for i in range(len(sticky_model_list)):\n",
    "    inpts = test_data_x[i]['data'].to_numpy()\n",
    "    choices = test_choices[i]['choices']\n",
    "    sticky_test_ll.append({'mouse': test_data_x[i]['mouse'], 'test_ll': sticky_model_list[i]['glmhmm'].log_likelihood(choices, inputs=inpts)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop through each mouse and find generative weights and append to list\n",
    "sticky_weights = []\n",
    "for i in range(len(sticky_model_list)):\n",
    "    sticky_weights.append({'mouse': train_data_x[i]['mouse'], 'weights': sticky_model_list[i]['glmhmm'].observations.params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sticky_G_weights = stickyG_model_list[0].observations.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = 1\n",
    "#model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8', '#e41a1c']\n",
    "# Plot the GLM weights for each input regressor for each mouse \n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for i in range(len(sticky_weights)):\n",
    "    for j in range(num_states_2plot):\n",
    "        plt.plot(-sticky_weights[i]['weights'][j,0,:], lw=2,\n",
    "             color=cols[j], marker = 'o', markersize = 5)\n",
    "        plt.plot(-sticky_G_weights[j,0,:], lw=2, color='black', linestyle='--')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n",
    "plt.title('Sticky HMM 1-state animal fits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create lists of weights seperated by age group\n",
    "weights_1 = []\n",
    "weights_2 = []\n",
    "weights_3 = []\n",
    "weights_4 = []\n",
    "\n",
    "for i in range(len(sticky_weights)):\n",
    "    if sticky_weights[i]['mouse'] in age_group_counts[1]:\n",
    "        weights_1.append(sticky_weights[i]['weights'])\n",
    "    elif sticky_weights[i]['mouse'] in age_group_counts[2]:\n",
    "        weights_2.append(sticky_weights[i]['weights'])\n",
    "    elif sticky_weights[i]['mouse'] in age_group_counts[3]:\n",
    "        weights_3.append(sticky_weights[i]['weights'])\n",
    "    elif sticky_weights[i]['mouse'] in age_group_counts[4]:\n",
    "        weights_4.append(sticky_weights[i]['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the weights for each input regressor for each mouse differ color for animals in different age_group_counts\n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "colors = ['lightcoral', 'red', 'darkred', 'brown']\n",
    "labels = ['2 months', '4 months', '6 months', '12 months']\n",
    "\n",
    "for i in range(len(weights_1)):\n",
    "    for j in range(num_states_2plot):\n",
    "        plt.plot(-weights_1[i][j,0,:], lw=2, color=colors[0], marker = 'o', markersize = 5, label = labels[0])\n",
    "for i in range(len(weights_2)):\n",
    "    for j in range(num_states_2plot):\n",
    "        plt.plot(-weights_2[i][j,0,:], lw=2, color=colors[1], marker = 'o', markersize = 5, label = labels[1])\n",
    "for i in range(len(weights_3)):\n",
    "    for j in range(num_states_2plot):\n",
    "        plt.plot(-weights_3[i][j,0,:], lw=2, color=colors[2], marker = 'o', markersize = 5, label = labels[2])\n",
    "for i in range(len(weights_4)):\n",
    "    for j in range(num_states_2plot):\n",
    "        plt.plot(-weights_4[i][j,0,:], lw=2, color=colors[3], marker = 'o', markersize = 5, label = labels[3])\n",
    "        plt.plot(-sticky_G_weights[j,0,:], lw=2, color='black', linestyle='--')\n",
    "\n",
    "#plt.legend()\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n",
    "plt.title('Sticky HMM 1-state animal fits')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save stationary model list, train and test scores, and LL list\n",
    "import pickle\n",
    "\n",
    "model_params = {'num_states': num_states,\n",
    "                'obs_dim': obs_dim,\n",
    "                'input_dim': input_dim,\n",
    "                'num_categories': num_categories,\n",
    "                'input_dim': input_dim,\n",
    "                'C': C,\n",
    "                'prior_sigma': prior_sigma,\n",
    "                'prior_alpha': prior_alpha,\n",
    "                'observations': \"input_driven_obs\",\n",
    "                'x_cols': x_cols,\n",
    "                'data_filter': 'age_group:1-4, probs:80-20, trial_based:1',\n",
    "                'notes': \"used 3 nback data, 80-20 prob, 90-10 split, 1 state, reward is 0-1, choice is -1-1, animal fits\"}\n",
    "\n",
    "data_splits = {'train_split': train_split,\n",
    "               'train_ids': train_ids,\n",
    "               'test_ids': test_ids,}\n",
    "               #'splits': splits}\n",
    "\n",
    "#create dictionary for pickle\n",
    "stationary_dict = {'model_params' : model_params,\n",
    "                   'stickyG_model_list': stickyG_model_list,\n",
    "                   'stationaryG_train_scores': stickyG_train_scores,\n",
    "                   'stationaryG_test_scores': stickyG_test_scores,\n",
    "                   'stationaryG_ll_list': stickyG_ll_list}\n",
    "\n",
    "#save dictionary in pickle file\n",
    "save_dir = r'C:\\Users\\janet\\Documents\\Behavior_samp_data'\n",
    "pickle.dump(stationary_dict, open(os.path.join(save_dir, 'sticky_group_fits.pkl'), 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##determine LL of held out test data\n",
    "# test_choices = test_data['Decision'].to_numpy()\n",
    "# test_choices = test_choices.reshape(-1, 1)\n",
    "# test_choices = test_choices.astype(int)\n",
    "\n",
    "# test_inpts = test_data_x.to_numpy()\n",
    "# sticky_test_ll_list = []\n",
    "\n",
    "# for ii in range(len(sticky_model_list)):\n",
    "#     sticky_test_ll_list.append(sticky_model_list[ii].log_likelihood(test_choices, inputs=test_inpts))\n",
    "#     print('Model with ' + str(num_states[ii]) + ' states has a test log likelihood of ' + str(sticky_test_ll_list[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalize log likelihood in test_glmhmm_list and test_ll_list\n",
    "sticky_norm_ll_list = []\n",
    "sticky_norm_test_ll_list = []\n",
    "for ii in range(len(sticky_ll_list)):\n",
    "    sticky_norm_ll_list.append([k / num_trials_per_train_sess for k in sticky_ll_list[ii] if k is not None])\n",
    "    sticky_norm_test_ll_list.append(sticky_test_ll_list[ii]/num_trials_per_test_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot subplots of the log probabilities of the train and test models. Fit model final LL should be greater\n",
    "# than or equal to true LL.\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "for ii in range(len(sticky_ll_list)):\n",
    "    axs[ii].plot(sticky_norm_ll_list[ii], label=\"EM\")\n",
    "    axs[ii].plot([0, len(sticky_norm_ll_list[ii])], sticky_norm_test_ll_list[ii] * np.ones(2), ':k', label=\"Test\")\n",
    "    axs[ii].legend(loc=\"lower right\")\n",
    "    axs[ii].set_xlabel(\"EM Iteration\")\n",
    "    axs[ii].set_xlim(0, len(sticky_norm_ll_list[ii]))\n",
    "    axs[ii].set_ylabel(\"Log Probability\")\n",
    "    axs[ii].set_title('Model with ' + str(num_states[ii]) + ' states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert to bits per state for train data\n",
    "sticky_bits_per_state_train = []\n",
    "for ii in range(len(sticky_model_list)):\n",
    "    import math\n",
    "    bits_per_sess = ((sticky_model_list[ii].log_likelihood(choices, inputs=inpts)-(np.log(0.5)*num_trials_per_train_sess))/(num_trials_per_train_sess*np.log(2)))\n",
    "    sticky_bits_per_state_train.append(bits_per_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert to bits per state for test data\n",
    "sticky_bits_per_state_test = []\n",
    "for ii in range(len(sticky_model_list)):\n",
    "    import math\n",
    "    bits_per_sess = ((sticky_model_list[ii].log_likelihood(test_choices, inputs=test_inpts)-(np.log(0.5)*num_trials_per_test_sess))/(num_trials_per_test_sess*np.log(2)))\n",
    "    sticky_bits_per_state_test.append(bits_per_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot bits/state train vs test\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(num_states, sticky_bits_per_state_train, label=\"Train\", marker='o')\n",
    "plt.plot(num_states, sticky_bits_per_state_test, label=\"Test\", marker='o')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.ylabel(\"bits/trials\")\n",
    "plt.xticks(num_states)\n",
    "plt.title('Sticky HMM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate AIC  for train data normalized by number of trials\n",
    "sticky_AIC_train = []\n",
    "for ii in range(len(sticky_model_list)):\n",
    "    AIC = (-2*(sticky_model_list[ii].log_likelihood(choices, inputs=inpts)/num_trials_per_train_sess))+(2*(num_states[ii]))\n",
    "    sticky_AIC_train.append(AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate AIC for test data normalized by number of trials\n",
    "sticky_AIC_test = []\n",
    "for ii in range(len(sticky_model_list)):\n",
    "    AIC = (-2*(sticky_model_list[ii].log_likelihood(test_choices, inputs=test_inpts)/num_trials_per_test_sess))+(2*(num_states[ii]))\n",
    "    sticky_AIC_test.append(AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot AIC for train and test data side by side\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "plt.plot(num_states, sticky_AIC_train, label=\"AIC Train\", marker='o', color='green')\n",
    "plt.plot(num_states, sticky_AIC_test, label=\"AIC Test\", marker='o', color='magenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.title('Sticky HMM')\n",
    "plt.ylabel(\"AIC\")\n",
    "plt.xticks(num_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get expected states for each model:\n",
    "# expectations_list = []\n",
    "# for i in range(len(num_states)):\n",
    "#     expectations_list.append([sticky_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "#                 for data, inpt\n",
    "#                 in zip([choices], [inpts])])\n",
    "\n",
    "# ## Get expected states for held out test data for each model:\n",
    "# sticky_test_expectations_list = []\n",
    "# for i in range(len(num_states)):\n",
    "#     sticky_test_expectations_list.append([sticky_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "#                 for data, inpt\n",
    "#                 in zip([test_choices], [test_inpts])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop through each mouse to get expected states for each model\n",
    "sticky_expectations_list = []\n",
    "for i, mouse in enumerate(mouse_dfs):\n",
    "    inpts = train_data_x[i]['data'].to_numpy()\n",
    "    choices = train_choices[i]['choices']\n",
    "    sticky_expectations_list.append({'mouse': train_data[i]['mouse']['mouse'], 'expectations': [sticky_model_list[i]['glmhmm'].expected_states(data=choices, input=inpts)[0]]})\n",
    "\n",
    "sticky_test_expectations_list = []\n",
    "for i, mouse in enumerate(mouse_dfs):\n",
    "    inpts = test_data_x[i]['data'].to_numpy()\n",
    "    choices = test_choices[i]['choices']\n",
    "    sticky_test_expectations_list.append({'mouse': test_data[i]['mouse']['mouse'], 'expectations': [sticky_model_list[i]['glmhmm'].expected_states(data=choices, input=inpts)[0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate predictive accuracy for each model\n",
    "sticky_predictive_acc_list = []\n",
    "for i in range(len(num_states)):\n",
    "    model = sticky_model_list[i]\n",
    "    expectations = sticky_test_expectations_list[i]\n",
    "    glm_weights = -model.observations.params\n",
    "    permutation = np.argsort(glm_weights[:, 0, 0])\n",
    "    masks = [np.ones_like(data, dtype=bool) for data in [test_choices]]\n",
    "    # Convert this now to one array:\n",
    "    posterior_probs = np.concatenate(expectations, axis=0)\n",
    "    posterior_probs = posterior_probs[:, permutation]\n",
    "    prob_right = [\n",
    "        np.exp(model.observations.calculate_logits(input=test_inpts))\n",
    "        for data, input, train_mask in zip(test_choices, test_inpts, masks)\n",
    "    ]\n",
    "    prob_right = np.concatenate(prob_right, axis=0)\n",
    "    # Now multiply posterior probs and prob_right:\n",
    "    prob_right = prob_right[:, :, 1] ## taking logits running an exponential then arranging in a 3d array\n",
    "    # Now multiply posterior probs and prob_right and sum over latent axis:\n",
    "    final_prob_right = np.sum(np.multiply(posterior_probs, prob_right), axis=1)\n",
    "    # Get the predicted label for each time step:\n",
    "    predicted_label = np.around(final_prob_right, decimals=0).astype('int')\n",
    "    # Examine at appropriate idx\n",
    "    predictive_acc = np.sum(test_choices[:, 0] == predicted_label) / len(test_choices)\n",
    "    print('Model with ' + str(num_states[i]) + ' state(s) has a test predictive accuracy of ' + str(predictive_acc))\n",
    "    sticky_predictive_acc_list.append(predictive_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot plot of predictive accuracy for each model\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(num_states, sticky_predictive_acc_list, marker='o', color='magenta')\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.ylabel(\"Predictive Accuracy - Test Data\")\n",
    "plt.ylim(0.5, 1)\n",
    "plt.xticks(num_states)\n",
    "plt.title('Sticky Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = 3\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "\n",
    "sticky_gen_weights = sticky_model_list[model_to_plot].observations.params #select correct model whether in list, or just test_glmhmm\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8', '#e41a1c']\n",
    "# Plot the GLM weights for each input regressor against each state for the model with 2 states\n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for k in range(num_states_2plot):\n",
    "    plt.plot(sticky_gen_weights[k,0,:], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k], marker = 'o', markersize = 5)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n",
    "plt.title('Sticky HMM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting transition matrix: Here, we will look at the resulting transition matrix. You can select any model output from the list below to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = 3\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "gen_weights = sticky_model_list[model_to_plot].observations.params #select correct model whether in list, or just test_glmhmm\n",
    "gen_trans_mat = sticky_model_list[model_to_plot].transitions.params #select correct model whether in list, or just test_glmhmm\n",
    "\n",
    "\n",
    "# Plot generative parameters:\n",
    "fig = plt.figure(figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "gen_trans_mat = np.exp(gen_trans_mat)[0]\n",
    "plt.imshow(gen_trans_mat, vmin=-0.8, vmax=1, cmap='bone')\n",
    "for i in range(gen_trans_mat.shape[0]):\n",
    "    for j in range(gen_trans_mat.shape[1]):\n",
    "        text = plt.text(j, i, str(np.around(gen_trans_mat[i, j], decimals=2)), ha=\"center\", va=\"center\",\n",
    "                        color=\"k\", fontsize=12)\n",
    "plt.xlim(-0.5, num_states_2plot - 0.5)\n",
    "plt.xticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.yticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.ylim(num_states_2plot - 0.5, -0.5)\n",
    "plt.ylabel(\"state t\", fontsize = 15)\n",
    "plt.xlabel(\"state t+1\", fontsize = 15)\n",
    "plt.title(\"Generative transition matrix - Sticky\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all models to get posterior probabilities for train data and state_occupancies\n",
    "posterior_probs_list = []\n",
    "state_occupancies_list = []\n",
    "for i in range(len(num_states)):\n",
    "    posterior_probs = [sticky_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])]\n",
    "    posterior_probs_concat = np.concatenate(posterior_probs)\n",
    "    state_max_posterior = np.argmax(posterior_probs_concat, axis = 1)\n",
    "    _, state_occupancies = np.unique(state_max_posterior, return_counts=True)\n",
    "    state_occupancies = state_occupancies/np.sum(state_occupancies)\n",
    "    posterior_probs_list.append(posterior_probs)\n",
    "    state_occupancies_list.append(state_occupancies)\n",
    "\n",
    "#Loop through all models to get posterior probabilities for test data and state_occupancies\n",
    "sticky_test_posterior_probs_list = []\n",
    "sticky_test_state_occupancies_list = []\n",
    "for i in range(len(num_states)):\n",
    "    sticky_test_posterior_probs = [sticky_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([test_choices], [test_inpts])]\n",
    "    sticky_test_posterior_probs_concat = np.concatenate(sticky_test_posterior_probs)\n",
    "    sticky_test_state_max_posterior = np.argmax(sticky_test_posterior_probs_concat, axis = 1)\n",
    "    _, sticky_test_state_occupancies = np.unique(sticky_test_state_max_posterior, return_counts=True)\n",
    "    sticky_test_state_occupancies = sticky_test_state_occupancies/np.sum(sticky_test_state_occupancies)\n",
    "    sticky_test_posterior_probs_list.append(sticky_test_posterior_probs)\n",
    "    sticky_test_state_occupancies_list.append(sticky_test_state_occupancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_to_plot = (0, 500) # number of trials to plot\n",
    "num_states_2plot = 3\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "sess_id = 0 #session id; can choose any index between 0 and num_sess-1\n",
    "\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8', '#e41a1c']\n",
    "fig = plt.figure(figsize=(10, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "for k in range(num_states_2plot):\n",
    "    posterior_probs = posterior_probs_list[model_to_plot]\n",
    "    plt.plot(posterior_probs[sess_id][:, k], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k])\n",
    "plt.ylim((-0.01, 1.01))\n",
    "plt.yticks([0, 0.5, 1], fontsize = 10)\n",
    "plt.xlim(trials_to_plot) \n",
    "plt.xlabel(\"trial #\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"p(state)\", fontsize = 15)\n",
    "plt.title('Sticky HMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = 3\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "fig = plt.figure(figsize=(2, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for z, occ in enumerate(sticky_test_state_occupancies_list[model_to_plot]):\n",
    "    plt.bar(z, occ, width = 0.8, color = cols[z])\n",
    "plt.ylim((0, 1))\n",
    "plt.xticks(range(num_states_2plot), range(num_states_2plot), fontsize = 10)\n",
    "plt.yticks([0, 0.5, 1], ['0', '0.5', '1'], fontsize=10)\n",
    "plt.xlabel('state', fontsize = 15)\n",
    "plt.ylabel('frac. occupancy', fontsize=15)\n",
    "plt.title('Sticky HMM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Option 2 -- Stationary HMM -- input driven, looping through states. Much the same as above except, we are changing the transitions to 'standard'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL FITS\n",
    "stationaryG_model_list = []\n",
    "stationaryG_ll_list = []\n",
    "stationaryG_train_scores = []\n",
    "stationaryG_test_scores = []\n",
    "for i in range(len(num_states)):\n",
    "    from ssm import model_selection\n",
    "    map_glmhmm = ssm.HMM(num_states[i], obs_dim, input_dim, observations=\"input_driven_obs\", \n",
    "                observation_kwargs=dict(C=num_categories, prior_sigma=prior_sigma),\n",
    "                transitions=\"standard\")\n",
    "    train_scores, test_scores = ssm.model_selection.cross_val_scores(map_glmhmm, choices, inpts, heldout_frac=0.1, n_repeats=5, verbose=True)\n",
    "    N_iters = 500\n",
    "    stationary_ll = map_glmhmm.fit(choices, inputs=inpts, method=\"em\", num_iters=N_iters, initialize=False)\n",
    "    stationaryG_ll_list.append(stationary_ll)\n",
    "    stationaryG_train_scores.append(train_scores)\n",
    "    stationaryG_test_scores.append(test_scores)\n",
    "    stationaryG_model_list.append(map_glmhmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationaryG_weights = stationaryG_model_list[0].observations.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop though each mouse for stationary model fits\n",
    "stationary_model_list = []\n",
    "stationary_ll_list = []\n",
    "stationary_train_scores = []\n",
    "stationary_test_scores = []\n",
    "\n",
    "for i, mouse in enumerate(mouse_dfs):\n",
    "    inpts = train_data_x[i]['data'].to_numpy()\n",
    "    choices = train_choices[i]['choices']\n",
    "    num_trials_per_sess = len(inpts)\n",
    "    num_sessions = len(train_data_x[i]['data'])\n",
    "    input_dim = inpts.shape[1]\n",
    "    for i in range(len(num_states)):\n",
    "        from ssm import model_selection\n",
    "        map_glmhmm = ssm.HMM(num_states[i], obs_dim, input_dim, observations=\"input_driven_obs\", \n",
    "                    observation_kwargs=dict(C=num_categories,prior_sigma=prior_sigma),\n",
    "                    transitions=\"standard\")\n",
    "        train_scores, test_scores = ssm.model_selection.cross_val_scores(map_glmhmm, choices, inpts, heldout_frac=0.1, n_repeats=3, verbose=True)\n",
    "        N_iters = 500\n",
    "        stationary_ll = map_glmhmm.fit(choices, inputs=inpts, method=\"em\", num_iters=N_iters, initialize=False)\n",
    "        stationary_ll_list.append({'mouse': train_data[i]['mouse']['mouse'], 'll': stationary_ll})\n",
    "        stationary_train_scores.append({'mouse': train_data[i]['mouse']['mouse'], 'scores': train_scores})\n",
    "        stationary_test_scores.append({'mouse': train_data[i]['mouse']['mouse'], 'scores':test_scores})\n",
    "        stationary_model_list.append({'mouse': train_data[i]['mouse']['mouse'], 'glmhmm': map_glmhmm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop through each mouse and determine test log likelihood\n",
    "stationary_test_ll = []\n",
    "for i in range(len(stationary_model_list)):\n",
    "    inpts = test_data_x[i]['data'].to_numpy()\n",
    "    choices = test_choices[i]['choices']\n",
    "    stationary_test_ll.append({'mouse': test_data_x[i]['mouse'], 'test_ll': stationary_model_list[i]['glmhmm'].log_likelihood(choices, inputs=inpts)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop through each mouse and find generative weights and append to list\n",
    "stationary_weights = []\n",
    "for i in range(len(stationary_model_list)):\n",
    "    stationary_weights.append({'mouse': train_data_x[i]['mouse'], 'weights': stationary_model_list[i]['glmhmm'].observations.params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save stationary model list, train and test scores, and LL list\n",
    "import pickle\n",
    "\n",
    "model_params = {'num_states': num_states,\n",
    "                'obs_dim': obs_dim,\n",
    "                'input_dim': input_dim,\n",
    "                'num_categories': num_categories,\n",
    "                'input_dim': input_dim,\n",
    "                'C': C,\n",
    "                'prior_sigma': prior_sigma,\n",
    "                'prior_alpha': prior_alpha,\n",
    "                'observations': \"input_driven_obs\",\n",
    "                'x_cols': x_cols,\n",
    "                'data_filter': 'age_group:1-4, probs:80-20, trial_based:1',\n",
    "                'notes': \"used 3 nback data, 80-20 prob, 90-10 split, 2 states, reward is 0-1, choice is -1-1, global fit\"}\n",
    "\n",
    "data_splits = {'train_split': train_split,\n",
    "               'train_ids': train_ids,\n",
    "               'test_ids': test_ids}\n",
    "\n",
    "#create dictionary for pickle\n",
    "stationary_dict = {'model_params' : model_params,\n",
    "                    'data_splits': data_splits,\n",
    "                   'stationary_model_list': stationaryG_model_list,\n",
    "                   'stationary_train_scores': stationaryG_train_scores,\n",
    "                   'stationary_test_scores': stationaryG_test_scores,\n",
    "                   'stationary_ll_list': stationaryG_ll_list}\n",
    "\n",
    "#save dictionary in pickle file\n",
    "save_dir = r'C:\\Users\\janet\\Documents\\Behavior_samp_data\\models'\n",
    "pickle.dump(stationary_dict, open(os.path.join(save_dir, 'stationary_group_fits.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = 2\n",
    "#model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8', '#e41a1c']\n",
    "# Plot the GLM weights for each input regressor for each mouse use subplots for each state\n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for i in range(len(stationary_weights)):\n",
    "    for j in range(num_states_2plot):\n",
    "        plt.plot(-stationary_weights[i]['weights'][j,0,:], lw=2,\n",
    "             color=cols[j], marker = 'o', markersize = 5)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n",
    "plt.title('Stationary HMM 2-state animal fits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the GLM weights for each input regressor for each mouse use subplots for each state\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "for i in range(len(stationary_weights)):\n",
    "    for j in range(num_states_2plot):\n",
    "        axs[j].plot(-stationary_weights[i]['weights'][j,0,:], lw=2,\n",
    "             color=cols[j], marker = 'o', markersize = 5)\n",
    "        axs[j].plot(-stationaryG_weights[j,0,:], lw=2, color='black', linestyle='--')\n",
    "        axs[j].set_title('State' + str(j + 1), fontsize = 15)\n",
    "        axs[j].set_ylabel(\"GLM weight\", fontsize = 15)\n",
    "        axs[j].set_xlabel(\"Inputs\", fontsize = 15)\n",
    "        axs[j].set_xlim((0, len(x_cols)))\n",
    "        axs[j].set_xticks(np.arange(len(x_cols)))\n",
    "        axs[j].set_xticklabels(x_cols, rotation=45, fontsize = 10)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model list, train and test scores, and LL list\n",
    "import pickle\n",
    "save_dir = r'C:\\Users\\janet\\Documents\\Behavior_samp_data\\models\\1-state'\n",
    "\n",
    "saved_model_dict = pickle.load(open(os.path.join(save_dir, 'sticky_group_fits.pkl'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calc standard error of the mean for train and test scores\n",
    "stationary_train_scores_sem = []\n",
    "for i in range(len(stationary_train_scores)):\n",
    "    stationary_train_scores_sem.append(np.std(stationary_train_scores[i])/np.sqrt(len(stationary_train_scores[i])))\n",
    "\n",
    "stationary_test_scores_sem = []\n",
    "for i in range(len(stationary_test_scores)):\n",
    "    stationary_test_scores_sem.append(np.std(stationary_test_scores[i])/np.sqrt(len(stationary_test_scores[i])))\n",
    "\n",
    "## determine critical value for set confidence interval\n",
    "from scipy.stats import norm\n",
    "confidence = 0.95\n",
    "alpha_2 = (1 - confidence) / 2\n",
    "critical_value = norm.ppf(1 - alpha_2)\n",
    "\n",
    "## calc confidence interval for train and test scores\n",
    "stationary_train_scores_ci = []\n",
    "for i in range(len(stationary_train_scores)):\n",
    "    stationary_train_scores_ci.append([np.mean(stationary_train_scores[i]) - (critical_value * stationary_train_scores_sem[i]),\n",
    "                                  np.mean(stationary_train_scores[i]) + (critical_value * stationary_train_scores_sem[i])])\n",
    "\n",
    "stationary_test_scores_ci = []\n",
    "for i in range(len(stationary_test_scores)):\n",
    "    stationary_test_scores_ci.append([np.mean(stationary_test_scores[i]) - (critical_value * stationary_test_scores_sem[i]),\n",
    "                                np.mean(stationary_test_scores[i]) + (critical_value * stationary_test_scores_sem[i])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot train and test scores for each model and display confidence intervals\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(num_states, [np.mean(s) for s in stationary_train_scores], label=\"Train\")\n",
    "plt.plot(num_states, [np.mean(s) for s in stationary_test_scores], label=\"Test\")\n",
    "plt.fill_between(num_states, [s[0] for s in stationary_train_scores_ci], [s[1] for s in stationary_train_scores_ci], alpha=0.3)\n",
    "plt.fill_between(num_states, [s[0] for s in stationary_test_scores_ci], [s[1] for s in stationary_test_scores_ci], alpha=0.3)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Number of states\")\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.title(\"Cross Validation Scores; error = confidence interval 95%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(stationaryG_model_list)):\n",
    "    log_likelihood = stationaryG_model_list[ii].log_likelihood(choices, inputs=inpts)\n",
    "    print('Model with ' + str(num_states[ii]) + ' states has a train log likelihood of ' + str(log_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##determine LL of held out test data\n",
    "test_choices = test_data['Decision'].to_numpy()\n",
    "test_choices = test_choices.reshape(-1, 1)\n",
    "test_choices = test_choices.astype(int)\n",
    "\n",
    "test_inpts = test_data_x.to_numpy()\n",
    "stationaryG_test_ll_list = []\n",
    "\n",
    "for ii in range(len(stationaryG_model_list)):\n",
    "    stationaryG_test_ll_list.append(stationaryG_model_list[ii].log_likelihood(test_choices, inputs=test_inpts))\n",
    "    print('Model with ' + str(num_states[ii]) + ' states has a test log likelihood of ' + str(stationaryG_test_ll_list[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalize log likelihood in test_glmhmm_list and test_ll_list\n",
    "stationaryG_norm_ll_list = []\n",
    "stationaryG_norm_test_ll_list = []\n",
    "for ii in range(len(stationaryG_ll_list)):\n",
    "    stationaryG_norm_ll_list.append([k / num_trials_per_train_sess for k in stationaryG_ll_list[ii] if k is not None])\n",
    "    stationaryG_norm_test_ll_list.append((stationaryG_test_ll_list[ii]/num_trials_per_test_sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot subplots of the log probabilities of the train and test models. Fit model final LL should be greater\n",
    "# than or equal to true LL.\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "for ii in range(len(stationaryG_ll_list)):\n",
    "    axs[ii].plot(stationaryG_norm_ll_list[ii], label=\"EM\")\n",
    "    #axs[ii].plot([0, len(stationary_norm_ll_list[ii])], stationary_norm_test_ll_list[ii] * np.ones(2), ':k', label=\"Test\")\n",
    "    axs[ii].legend(loc=\"lower right\")\n",
    "    axs[ii].set_xlabel(\"EM Iteration\")\n",
    "    axs[ii].set_xlim(0, len(stationaryG_norm_ll_list[ii]))\n",
    "    axs[ii].set_ylabel(\"Log Probability\")\n",
    "    axs[ii].set_title('Model with ' + str(num_states[ii]) + ' states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert to bits per state for train data\n",
    "stationary_bits_per_state_train = []\n",
    "for ii in range(len(stationary_model_list)):\n",
    "    import math\n",
    "    bits_per_sess = ((stationary_model_list[ii].log_likelihood(choices, inputs=inpts)-(np.log(0.5)*num_trials_per_train_sess))/(num_trials_per_train_sess*np.log(2)))\n",
    "    stationary_bits_per_state_train.append(bits_per_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert to bits per state for test data\n",
    "stationary_bits_per_state_test = []\n",
    "for ii in range(len(stationary_model_list)):\n",
    "    import math\n",
    "    bits_per_sess = ((stationary_model_list[ii].log_likelihood(test_choices, inputs=test_inpts)-(np.log(0.5)*num_trials_per_test_sess))/(num_trials_per_test_sess*np.log(2)))\n",
    "    stationary_bits_per_state_test.append(bits_per_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot bits/state train vs test\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(num_states, stationary_bits_per_state_train, label=\"Train\", marker='o')\n",
    "#plt.plot(num_states, stationary_bits_per_state_test, label=\"Test\", marker='o')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.ylabel(\"bits/trials\")\n",
    "plt.xticks(num_states)\n",
    "plt.title('Standard HMM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate AIC  for train data normalized by number of trials\n",
    "stationary_AIC_train = []\n",
    "for ii in range(len(stationary_model_list)):\n",
    "    AIC = (-2*(stationary_model_list[ii].log_likelihood(choices, inputs=inpts)/num_trials_per_train_sess))+(2*(num_states[ii]))\n",
    "    stationary_AIC_train.append(AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate AIC  for train data normalized by number of trials\n",
    "stationary_AIC_test = []\n",
    "for ii in range(len(stationary_model_list)):\n",
    "    AIC = (-2*(stationary_model_list[ii].log_likelihood(test_choices, inputs=test_inpts)/num_trials_per_test_sess))+(2*(num_states[ii]))\n",
    "    stationary_AIC_test.append(AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot AIC for train and test data side by side\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "plt.plot(num_states, stationary_AIC_train, label=\"AIC Train\", marker='o', color='green')\n",
    "plt.plot(num_states, stationary_AIC_test, label=\"AIC Test\", marker='o', color='magenta')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.title('Standard HMM')\n",
    "plt.ylabel(\"AIC\")\n",
    "plt.xticks(num_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get expected states for each model:\n",
    "expectations_list = []\n",
    "for i in range(len(num_states)):\n",
    "    expectations_list.append([stationary_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])])\n",
    "    \n",
    "## Get expected states for held out test data for each model:\n",
    "stationary_test_expectations_list = []\n",
    "for i in range(len(num_states)):\n",
    "    stationary_test_expectations_list.append([stationary_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([test_choices], [test_inpts])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicitve accuracy for test data for each model\n",
    "stationary_test_predictive_acc_list = []\n",
    "for i in range(len(num_states)):\n",
    "    model = stationary_model_list[i]\n",
    "    expectations = stationary_test_expectations_list[i]\n",
    "    glm_weights = -model.observations.params\n",
    "    permutation = np.argsort(glm_weights[:, 0, 0])\n",
    "    masks = [np.ones_like(data, dtype=bool) for data in [test_choices]]\n",
    "    # Convert this now to one array:\n",
    "    posterior_probs = np.concatenate(expectations, axis=0)\n",
    "    posterior_probs = posterior_probs[:, permutation]\n",
    "    prob_right = [\n",
    "        np.exp(model.observations.calculate_logits(input=test_inpts))\n",
    "        for data, input, train_mask in zip(test_choices, test_inpts, masks)\n",
    "    ]\n",
    "    prob_right = np.concatenate(prob_right, axis=0)\n",
    "    # Now multiply posterior probs and prob_right:\n",
    "    prob_right = prob_right[:, :, 1] ## taking logits running an exponential then arranging in a 3d array\n",
    "    # Now multiply posterior probs and prob_right and sum over latent axis:\n",
    "    final_prob_right = np.sum(np.multiply(posterior_probs, prob_right), axis=1)\n",
    "    # Get the predicted label for each time step:\n",
    "    predicted_label = np.around(final_prob_right, decimals=0).astype('int')\n",
    "    # Examine at appropriate idx\n",
    "    predictive_acc = np.sum(test_choices[:, 0] == predicted_label) / len(test_choices)\n",
    "    print('Model with ' + str(num_states[i]) + ' state(s) has a test predictive accuracy of ' + str(predictive_acc))\n",
    "    stationary_test_predictive_acc_list.append(predictive_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot plot of predictive accuracy for each model\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(num_states, stationary_test_predictive_acc_list, marker='o', color='magenta')\n",
    "plt.xlabel(\"Number of States\")\n",
    "plt.ylabel(\"Predictive Accuracy - Test Data\")\n",
    "plt.ylim(0.3, 1)\n",
    "plt.xticks(num_states)\n",
    "plt.title('Standard Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = 2\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "stationary_gen_weights = -stationary_model_list[model_to_plot].observations.params #select correct model whether in list, or just test_glmhmm\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8', '#e41a1c']\n",
    "# Plot the GLM weights for each input regressor against each state for the model with 2 states\n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for k in range(num_states_2plot):\n",
    "    plt.plot(stationary_gen_weights[k,0,:], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k], marker = 'o', markersize = 5)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n",
    "plt.title('Standard HMM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all models to get posterior probabilities for train data and state_occupancies\n",
    "posterior_probs_list = []\n",
    "state_occupancies_list = []\n",
    "for i in range(len(num_states)):\n",
    "    posterior_probs = [stationary_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])]\n",
    "    posterior_probs_concat = np.concatenate(posterior_probs)\n",
    "    state_max_posterior = np.argmax(posterior_probs_concat, axis = 1)\n",
    "    _, state_occupancies = np.unique(state_max_posterior, return_counts=True)\n",
    "    state_occupancies = state_occupancies/np.sum(state_occupancies)\n",
    "    posterior_probs_list.append(posterior_probs)\n",
    "    state_occupancies_list.append(state_occupancies)\n",
    "\n",
    "#Loop through all models to get posterior probabilities for test data and state_occupancies\n",
    "stationary_test_posterior_probs_list = []\n",
    "stationary_test_state_occupancies_list = []\n",
    "for i in range(len(num_states)):\n",
    "    stationary_test_posterior_probs = [stationary_model_list[i].expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([test_choices], [test_inpts])]\n",
    "    stationary_test_posterior_probs_concat = np.concatenate(stationary_test_posterior_probs)\n",
    "    stationary_test_state_max_posterior = np.argmax(stationary_test_posterior_probs_concat, axis = 1)\n",
    "    _, stationary_test_state_occupancies = np.unique(stationary_test_state_max_posterior, return_counts=True)\n",
    "    stationary_test_state_occupancies = stationary_test_state_occupancies/np.sum(stationary_test_state_occupancies)\n",
    "    stationary_test_posterior_probs_list.append(stationary_test_posterior_probs)\n",
    "    stationary_test_state_occupancies_list.append(stationary_test_state_occupancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_to_plot = (0, 1000) # number of trials to plot\n",
    "num_states_2plot = 4\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "sess_id = 0 #session id; can choose any index between 0 and num_sess-1\n",
    "\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8', '#e41a1c']\n",
    "fig = plt.figure(figsize=(10, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "for k in range(num_states_2plot):\n",
    "    posterior_probs = posterior_probs_list[model_to_plot]\n",
    "    plt.plot(posterior_probs[sess_id][:, k], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k])\n",
    "plt.ylim((-0.01, 1.01))\n",
    "plt.yticks([0, 0.5, 1], fontsize = 10)\n",
    "plt.xlim(trials_to_plot) \n",
    "plt.xlabel(\"trial #\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"p(state)\", fontsize = 15)\n",
    "plt.title('Standard HMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = 4\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "fig = plt.figure(figsize=(2, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for z, occ in enumerate(stationary_test_state_occupancies_list[model_to_plot]):\n",
    "    plt.bar(z, occ, width = 0.8, color = cols[z])\n",
    "plt.ylim((0, 1))\n",
    "plt.xticks(range(num_states_2plot), range(num_states_2plot), fontsize = 10)\n",
    "plt.yticks([0, 0.5, 1], ['0', '0.5', '1'], fontsize=10)\n",
    "plt.xlabel('state', fontsize = 15)\n",
    "plt.ylabel('frac. occupancy', fontsize=15)\n",
    "plt.title('Standard HMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot transition matrix\n",
    "num_states_2plot = 4\n",
    "model_to_plot = (num_states_2plot-1) #remember, 0 indexed\n",
    "\n",
    "gen_weights = stationary_model_list[model_to_plot].observations.params #select correct model whether in list, or just test_glmhmm\n",
    "gen_trans_mat = stationary_model_list[model_to_plot].transitions.params #select correct model whether in list, or just test_glmhmm\n",
    "\n",
    "\n",
    "# Plot generative parameters:\n",
    "fig = plt.figure(figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "gen_trans_mat = np.exp(gen_trans_mat)[0]\n",
    "plt.imshow(gen_trans_mat, vmin=-0.8, vmax=1, cmap='bone')\n",
    "for i in range(gen_trans_mat.shape[0]):\n",
    "    for j in range(gen_trans_mat.shape[1]):\n",
    "        text = plt.text(j, i, str(np.around(gen_trans_mat[i, j], decimals=2)), ha=\"center\", va=\"center\",\n",
    "                        color=\"k\", fontsize=12)\n",
    "plt.xlim(-0.5, num_states_2plot - 0.5)\n",
    "plt.xticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.yticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.ylim(num_states_2plot - 0.5, -0.5)\n",
    "plt.ylabel(\"state t\", fontsize = 15)\n",
    "plt.xlabel(\"state t+1\", fontsize = 15)\n",
    "plt.title(\"Generative transition matrix - Standard\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. We've been looping through a list of states, but we can also just select a single state and run the model on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Fit GLM-HMM and perform recovery analysis -- this should come after the model selection section above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data inputs \n",
    "choices = train_data['Decision'].to_numpy()\n",
    "choices = choices.reshape(-1, 1)\n",
    "choices = choices.astype(int)\n",
    "\n",
    "inpts = train_data_x.to_numpy()\n",
    "\n",
    "# Model Inputs\n",
    "num_states = 3 # number of discrete states\n",
    "obs_dim =  1 # number of observed dimensions, 1 for just reward, 2 if you had something like reaction time\n",
    "num_categories = 2 # number of categories for the output\n",
    "input_dim = inpts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_glmhmm = ssm.HMM(num_states, obs_dim, input_dim, observations=\"input_driven_obs\", \n",
    "                   observation_kwargs=dict(C=num_categories), transitions=\"standard\") # transitions usually standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform cross-validation\n",
    "from ssm import model_selection\n",
    "test_scores, train_scores = ssm.model_selection.cross_val_scores(new_glmhmm, choices, inpts, heldout_frac=0.1, n_repeats=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iters = 200 # maximum number of EM iterations. Fitting with stop earlier if increase in LL is below tolerance specified by tolerance parameter\n",
    "fit_ll = new_glmhmm.fit(choices, inputs=inpts, method='em', num_iters=N_iters, tolerance=10**-4) #method=\"em\",\n",
    "print(\"EM converged: \", fit_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit to test data\n",
    "test_choices = test_data['Decision'].to_numpy()\n",
    "test_choices = test_choices.reshape(-1, 1)\n",
    "test_choices = test_choices.astype(int)\n",
    "\n",
    "test_inpts = test_data_x.to_numpy()\n",
    "\n",
    "test_ll = new_glmhmm.log_likelihood(test_choices, inputs=test_inpts)\n",
    "print(\"Test LL: \", test_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the log likelihood\n",
    "norm_ll = [s / num_trials_per_train_sess for s in fit_ll if s is not None]\n",
    "norm_test = test_ll / num_trials_per_test_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log probabilities of the train and test models. Fit model final LL should be greater \n",
    "# than or equal to true LL.\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(norm_ll, label=\"EM\")\n",
    "plt.plot([0, len(norm_ll)], norm_test * np.ones(2), ':k', label=\"Test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"EM Iteration\")\n",
    "plt.xlim(0, len(fit_ll))\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Posterior State Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot $p(z_{t} = k|\\mathbf{y}, \\{u_{t}\\}_{t=1}^{T})$, the posterior state probabilities, which give the probability of the animal being in state k at trial t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = inpts.shape[0] # number of trials\n",
    "K = 2 # number of latent states\n",
    "C = 2 # number of observation classes or # C represents the binary choice the animal must make\n",
    "D = inpts.shape[1] # number of GLM inputs (regressors)\n",
    "\n",
    "# # Get expected states:\n",
    "posterior_probs = [new_glmhmm.expected_states(data=data, input=inpt)[0]\n",
    "                for data, inpt\n",
    "                in zip([choices], [inpts])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_to_plot = (0, 300) # number of trials to plot\n",
    "\n",
    "cols = ['#ff7f00', '#4daf4a', '#377eb8']\n",
    "fig = plt.figure(figsize=(10, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "sess_id = 0 #session id; can choose any index between 0 and num_sess-1\n",
    "\n",
    "for k in range(num_states):\n",
    "    plt.plot(posterior_probs[sess_id][:, k], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k])\n",
    "plt.ylim((-0.01, 1.01))\n",
    "plt.yticks([0, 0.5, 1], fontsize = 10)\n",
    "plt.xlim(trials_to_plot) \n",
    "plt.xlabel(\"trial #\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"p(state)\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate posterior probabilities across sessions\n",
    "posterior_probs_concat = np.concatenate(posterior_probs)\n",
    "# get state with maximum posterior probability at particular trial:\n",
    "state_max_posterior = np.argmax(posterior_probs_concat, axis = 1)\n",
    "# now obtain state fractional occupancies:\n",
    "_, state_occupancies = np.unique(state_max_posterior, return_counts=True)\n",
    "state_occupancies = state_occupancies/np.sum(state_occupancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for z, occ in enumerate(state_occupancies):\n",
    "    plt.bar(z, occ, width = 0.8, color = cols[z])\n",
    "plt.ylim((0, 1))\n",
    "plt.xticks([0, 1, 2], ['1', '2', '3'], fontsize = 10)\n",
    "plt.yticks([0, 0.5, 1], ['0', '0.5', '1'], fontsize=10)\n",
    "plt.xlabel('state', fontsize = 15)\n",
    "plt.ylabel('frac. occupancy', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gen_weights = new_glmhmm.observations.params\n",
    "\n",
    "#Plot the weights for each input regressor against each state\n",
    "fig = plt.figure(figsize=(7, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "for k in range(num_states):\n",
    "    plt.plot(model_gen_weights[k,0,:], label=\"State \" + str(k + 1), lw=2,\n",
    "             color=cols[k], marker = 'o', markersize = 5)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xticks(np.arange(len(x_cols)), x_cols, rotation=45, fontsize = 10)\n",
    "plt.ylabel(\"GLM weight\", fontsize = 15)\n",
    "plt.xlabel(\"Inputs\", fontsize = 15)\n",
    "plt.xlim((0, len(x_cols)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2plot = num_states\n",
    "gen_weights = new_glmhmm.observations.params #select correct model whether in list, or just test_glmhmm\n",
    "gen_trans_mat = new_glmhmm.transitions.params #select correct model whether in list, or just test_glmhmm\n",
    "\n",
    "\n",
    "# Plot generative parameters:\n",
    "fig = plt.figure(figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "gen_trans_mat = np.exp(gen_trans_mat)[0]\n",
    "plt.imshow(gen_trans_mat, vmin=-0.8, vmax=1, cmap='bone')\n",
    "for i in range(gen_trans_mat.shape[0]):\n",
    "    for j in range(gen_trans_mat.shape[1]):\n",
    "        text = plt.text(j, i, str(np.around(gen_trans_mat[i, j], decimals=2)), ha=\"center\", va=\"center\",\n",
    "                        color=\"k\", fontsize=12)\n",
    "plt.xlim(-0.5, num_states_2plot - 0.5)\n",
    "plt.xticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.yticks(range(0, num_states_2plot), range(num_states_2plot), fontsize=10)\n",
    "plt.ylim(num_states_2plot - 0.5, -0.5)\n",
    "plt.ylabel(\"state t\", fontsize = 15)\n",
    "plt.xlabel(\"state t+1\", fontsize = 15)\n",
    "plt.title(\"Generative transition matrix\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[:, 'State_Occupancy'] = state_max_posterior # State_Max_Posterior contains the state for each given trial \n",
    "\n",
    "#save new csv\n",
    "#filtered_data.to_csv(r'C:\\Users\\janet\\Documents\\Behavior_samp_data\\FullMerged_2nBack_100423.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabatini-glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
